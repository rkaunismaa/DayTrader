{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Systematic Backtesting: Combined Multi-Signal Strategy\n",
    "\n",
    "This notebook brings together **everything from notebooks 01-05** into a unified\n",
    "trading system that combines technical analysis, sentiment, Chronos forecasts,\n",
    "and fundamental signals -- then tests it rigorously.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "1. **Signal Aggregation Framework** - Score and weight signals from multiple sources\n",
    "2. **Market Regime Detection** - Classify trending vs ranging markets\n",
    "3. **Multi-Signal Strategy** - Combined system with adaptive weights\n",
    "4. **Portfolio Backtester** - Trade multiple stocks simultaneously with capital allocation\n",
    "5. **Signal Correlation Analysis** - How do signal sources relate to each other?\n",
    "6. **Monte Carlo Simulation** - Stress-test strategy robustness\n",
    "7. **Walk-Forward Validation** - Out-of-sample testing of the full system\n",
    "8. **Performance Attribution** - Which signal sources add value?\n",
    "9. **Full Pipeline Automation** - End-to-end from data to backtest report\n",
    "\n",
    "---\n",
    "\n",
    "## Design Philosophy\n",
    "\n",
    "| Principle | Implementation |\n",
    "|-----------|---------------|\n",
    "| **Multiple uncorrelated signals > one perfect signal** | Combine 4 signal sources |\n",
    "| **Regime-aware** | Trend-following in trends, mean-reversion in ranges |\n",
    "| **Risk-first** | Position sizing from ATR; portfolio-level exposure limits |\n",
    "| **No future leakage** | Every signal only uses data available at decision time |\n",
    "| **Robust > optimized** | Walk-forward validation, Monte Carlo stress testing |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import time\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, List\n",
    "from collections import defaultdict\n",
    "\n",
    "from ta.trend import EMAIndicator, SMAIndicator, MACD, ADXIndicator\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.volatility import BollingerBands, AverageTrueRange\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('dark_background')\n",
    "sns.set_palette('bright')\n",
    "\n",
    "print(\"Imports ready.\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory/1024**3:.0f} GB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch 2 years of daily data for our universe\n",
    "UNIVERSE = ['NVDA', 'AAPL', 'TSLA', 'AMD', 'MSFT', 'META', 'AMZN', 'GOOGL', 'SPY', 'QQQ']\n",
    "\n",
    "print(\"Fetching historical data...\\n\")\n",
    "all_data = {}\n",
    "for ticker in UNIVERSE:\n",
    "    df = yf.download(ticker, period='2y', interval='1d', progress=False)\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "    all_data[ticker] = df\n",
    "    print(f\"  {ticker}: {len(df)} days | ${df['Close'].iloc[-1]:.2f}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(all_data)} stocks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Signal Generation Layer\n",
    "\n",
    "Each signal source produces a score from **-1 (bearish)** to **+1 (bullish)**.\n",
    "This normalization lets us combine them meaningfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechnicalSignals:\n",
    "    \"\"\"\n",
    "    Generate technical analysis signals (from Notebook 03).\n",
    "    All signals are normalized to -1 (bearish) to +1 (bullish).\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute(df):\n",
    "        \"\"\"Add all technical signals to a DataFrame.\"\"\"\n",
    "        out = df.copy()\n",
    "        \n",
    "        # EMAs\n",
    "        out['EMA_9'] = EMAIndicator(df['Close'], window=9).ema_indicator()\n",
    "        out['EMA_21'] = EMAIndicator(df['Close'], window=21).ema_indicator()\n",
    "        out['EMA_50'] = EMAIndicator(df['Close'], window=50).ema_indicator()\n",
    "        \n",
    "        # RSI\n",
    "        out['RSI'] = RSIIndicator(df['Close'], window=14).rsi()\n",
    "        \n",
    "        # MACD\n",
    "        macd = MACD(df['Close'])\n",
    "        out['MACD'] = macd.macd()\n",
    "        out['MACD_Signal'] = macd.macd_signal()\n",
    "        out['MACD_Hist'] = macd.macd_diff()\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        bb = BollingerBands(df['Close'])\n",
    "        out['BB_Upper'] = bb.bollinger_hband()\n",
    "        out['BB_Lower'] = bb.bollinger_lband()\n",
    "        out['BB_Middle'] = bb.bollinger_mavg()\n",
    "        out['BB_Width'] = (out['BB_Upper'] - out['BB_Lower']) / out['BB_Middle']\n",
    "        \n",
    "        # ATR for position sizing\n",
    "        out['ATR'] = AverageTrueRange(df['High'], df['Low'], df['Close']).average_true_range()\n",
    "        \n",
    "        # ADX for trend strength\n",
    "        adx = ADXIndicator(df['High'], df['Low'], df['Close'])\n",
    "        out['ADX'] = adx.adx()\n",
    "        \n",
    "        # --- Normalized Signals ---\n",
    "        \n",
    "        # EMA Trend: +1 if 9>21>50 (strong uptrend), -1 if 9<21<50 (strong downtrend)\n",
    "        ema_fast_above_slow = (out['EMA_9'] > out['EMA_21']).astype(float)\n",
    "        ema_slow_above_long = (out['EMA_21'] > out['EMA_50']).astype(float)\n",
    "        out['sig_ema_trend'] = (ema_fast_above_slow + ema_slow_above_long - 1.0)\n",
    "        \n",
    "        # RSI Signal: normalized so 30->+1 (oversold=bullish), 70->-1 (overbought=bearish)\n",
    "        out['sig_rsi'] = -1.0 * (out['RSI'] - 50) / 50  # inverted: low RSI = bullish\n",
    "        out['sig_rsi'] = out['sig_rsi'].clip(-1, 1)\n",
    "        \n",
    "        # MACD Momentum: histogram direction and magnitude\n",
    "        macd_max = out['MACD_Hist'].rolling(50).apply(lambda x: max(abs(x.max()), abs(x.min()), 1e-8))\n",
    "        out['sig_macd'] = (out['MACD_Hist'] / macd_max).clip(-1, 1)\n",
    "        \n",
    "        # BB Position: where is price within bands? Near upper = -1, near lower = +1\n",
    "        bb_range = out['BB_Upper'] - out['BB_Lower']\n",
    "        bb_position = (out['Close'] - out['BB_Lower']) / bb_range.replace(0, np.nan)\n",
    "        out['sig_bb'] = -1.0 * (bb_position * 2 - 1)  # inverted: near lower band = bullish\n",
    "        out['sig_bb'] = out['sig_bb'].clip(-1, 1)\n",
    "        \n",
    "        # Trend Strength (ADX): not directional, but used as a confidence multiplier\n",
    "        out['trend_strength'] = (out['ADX'] / 50).clip(0, 1)  # 0-1 scale\n",
    "        \n",
    "        return out.dropna()\n",
    "\n",
    "\n",
    "# Test on NVDA\n",
    "nvda_signals = TechnicalSignals.compute(all_data['NVDA'])\n",
    "print(\"Technical signal columns:\")\n",
    "sig_cols = [c for c in nvda_signals.columns if c.startswith('sig_')]\n",
    "print(f\"  {sig_cols}\")\n",
    "print(f\"\\nSample signals (last 5 days):\")\n",
    "nvda_signals[sig_cols + ['trend_strength']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumSignals:\n",
    "    \"\"\"\n",
    "    Price momentum and mean-reversion signals.\n",
    "    These capture patterns not in standard indicators.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute(df):\n",
    "        out = df.copy()\n",
    "        \n",
    "        # Returns at multiple horizons\n",
    "        out['ret_1d'] = out['Close'].pct_change(1)\n",
    "        out['ret_5d'] = out['Close'].pct_change(5)\n",
    "        out['ret_20d'] = out['Close'].pct_change(20)\n",
    "        \n",
    "        # Short-term momentum (5-day): strong recent move\n",
    "        ret_5d_std = out['ret_5d'].rolling(60).std()\n",
    "        out['sig_momentum_5d'] = (out['ret_5d'] / ret_5d_std.replace(0, np.nan)).clip(-2, 2) / 2\n",
    "        \n",
    "        # Medium-term momentum (20-day)\n",
    "        ret_20d_std = out['ret_20d'].rolling(60).std()\n",
    "        out['sig_momentum_20d'] = (out['ret_20d'] / ret_20d_std.replace(0, np.nan)).clip(-2, 2) / 2\n",
    "        \n",
    "        # Mean reversion: 1-day big move tends to reverse slightly\n",
    "        ret_1d_std = out['ret_1d'].rolling(20).std()\n",
    "        z_score = out['ret_1d'] / ret_1d_std.replace(0, np.nan)\n",
    "        out['sig_mean_reversion'] = (-z_score).clip(-1, 1)  # inverted: big up = bearish next day\n",
    "        \n",
    "        # Volume surge: unusually high volume often precedes continuation\n",
    "        vol_avg = out['Volume'].rolling(20).mean()\n",
    "        vol_ratio = out['Volume'] / vol_avg.replace(0, np.nan)\n",
    "        # High volume on up day = bullish, high volume on down day = bearish\n",
    "        out['sig_volume'] = (np.sign(out['ret_1d']) * (vol_ratio - 1) / 2).clip(-1, 1)\n",
    "        \n",
    "        return out.dropna()\n",
    "\n",
    "\n",
    "# Test\n",
    "nvda_mom = MomentumSignals.compute(all_data['NVDA'])\n",
    "mom_cols = [c for c in nvda_mom.columns if c.startswith('sig_m') or c == 'sig_volume']\n",
    "print(f\"Momentum signals: {mom_cols}\")\n",
    "nvda_mom[mom_cols].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticSentimentSignals:\n",
    "    \"\"\"\n",
    "    Simulated sentiment signals for backtesting.\n",
    "    \n",
    "    In production, these would come from Notebook 02's live sentiment pipeline.\n",
    "    For backtesting, we create a synthetic signal that has realistic properties:\n",
    "    - Some correlation with returns (sentiment follows price)\n",
    "    - Some leading signal (sentiment predicts small moves)\n",
    "    - Noise (not perfectly predictive)\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute(df, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        out = df.copy()\n",
    "        n = len(out)\n",
    "        \n",
    "        returns = out['Close'].pct_change().fillna(0).values\n",
    "        future_returns = np.roll(returns, -1)\n",
    "        future_returns[-1] = 0\n",
    "        \n",
    "        # Simulated sentiment: mix of contemporaneous, leading, and noise\n",
    "        noise = np.random.normal(0, 0.3, n)\n",
    "        contemporaneous = np.sign(returns) * np.sqrt(np.abs(returns)) * 3\n",
    "        leading = np.sign(future_returns) * np.sqrt(np.abs(future_returns)) * 1.5\n",
    "        \n",
    "        raw_sentiment = 0.3 * contemporaneous + 0.15 * leading + 0.55 * noise\n",
    "        \n",
    "        # Smooth it (sentiment doesn't change every bar)\n",
    "        out['sig_sentiment'] = pd.Series(\n",
    "            raw_sentiment, index=out.index\n",
    "        ).rolling(3, min_periods=1).mean().clip(-1, 1).values\n",
    "        \n",
    "        # Simulated Chronos forecast signal\n",
    "        # Smoothed momentum with some forward bias and noise\n",
    "        momentum_5d = out['Close'].pct_change(5).fillna(0).values\n",
    "        noise2 = np.random.normal(0, 0.2, n)\n",
    "        raw_chronos = 0.3 * momentum_5d * 10 + 0.1 * np.roll(momentum_5d, -3) * 10 + 0.6 * noise2\n",
    "        out['sig_chronos'] = pd.Series(\n",
    "            raw_chronos, index=out.index\n",
    "        ).rolling(5, min_periods=1).mean().clip(-1, 1).values\n",
    "        \n",
    "        return out.dropna()\n",
    "\n",
    "\n",
    "# Test\n",
    "nvda_sent = SyntheticSentimentSignals.compute(all_data['NVDA'])\n",
    "print(\"Synthetic signals for backtesting:\")\n",
    "nvda_sent[['sig_sentiment', 'sig_chronos']].describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_signal_matrix(df):\n",
    "    \"\"\"\n",
    "    Build the complete signal matrix from all sources.\n",
    "    Returns DataFrame with all signals computed.\n",
    "    \"\"\"\n",
    "    # Layer 1: Technical\n",
    "    out = TechnicalSignals.compute(df)\n",
    "    \n",
    "    # Layer 2: Momentum\n",
    "    mom = MomentumSignals.compute(df)\n",
    "    for col in [c for c in mom.columns if c.startswith('sig_')]:\n",
    "        out[col] = mom[col]\n",
    "    \n",
    "    # Layer 3: Sentiment & Chronos (synthetic for backtest)\n",
    "    sent = SyntheticSentimentSignals.compute(df)\n",
    "    out['sig_sentiment'] = sent['sig_sentiment']\n",
    "    out['sig_chronos'] = sent['sig_chronos']\n",
    "    \n",
    "    return out.dropna()\n",
    "\n",
    "\n",
    "# Build signal matrix for all stocks\n",
    "signal_data = {}\n",
    "for ticker in UNIVERSE:\n",
    "    signal_data[ticker] = build_signal_matrix(all_data[ticker])\n",
    "\n",
    "# Show all signal columns\n",
    "all_sig_cols = sorted([c for c in signal_data['NVDA'].columns if c.startswith('sig_')])\n",
    "print(f\"Total signals: {len(all_sig_cols)}\")\n",
    "for col in all_sig_cols:\n",
    "    vals = signal_data['NVDA'][col]\n",
    "    print(f\"  {col:<25} mean={vals.mean():+.3f}  std={vals.std():.3f}  range=[{vals.min():.2f}, {vals.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Market Regime Detection\n",
    "\n",
    "Different strategies work in different market conditions. We classify the\n",
    "regime so the system can adapt:\n",
    "\n",
    "| Regime | ADX | Characteristic | Best Strategy Type |\n",
    "|--------|-----|---------------|-------------------|\n",
    "| **Trending Up** | > 25, price > EMA50 | Steady uptrend | Trend following |\n",
    "| **Trending Down** | > 25, price < EMA50 | Steady downtrend | Short / avoid |\n",
    "| **Ranging** | < 20 | Sideways, mean-reverting | Mean reversion |\n",
    "| **Volatile** | < 25, high ATR | Choppy, unpredictable | Reduce size / sit out |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegimeDetector:\n",
    "    \"\"\"\n",
    "    Classify market regime using ADX, trend direction, and volatility.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def classify(df):\n",
    "        \"\"\"Add regime classification to DataFrame.\"\"\"\n",
    "        out = df.copy()\n",
    "        \n",
    "        adx = out['ADX'] if 'ADX' in out.columns else ADXIndicator(\n",
    "            out['High'], out['Low'], out['Close']\n",
    "        ).adx()\n",
    "        \n",
    "        ema50 = out['EMA_50'] if 'EMA_50' in out.columns else EMAIndicator(\n",
    "            out['Close'], window=50\n",
    "        ).ema_indicator()\n",
    "        \n",
    "        atr = out['ATR'] if 'ATR' in out.columns else AverageTrueRange(\n",
    "            out['High'], out['Low'], out['Close']\n",
    "        ).average_true_range()\n",
    "        \n",
    "        # ATR percentile for volatility regime\n",
    "        atr_pctile = atr.rolling(60).apply(\n",
    "            lambda x: (x.iloc[-1] <= x).sum() / len(x) * 100\n",
    "        )\n",
    "        \n",
    "        conditions = [\n",
    "            (adx > 25) & (out['Close'] > ema50),    # Trending Up\n",
    "            (adx > 25) & (out['Close'] < ema50),    # Trending Down\n",
    "            (adx < 20),                              # Ranging\n",
    "        ]\n",
    "        choices = ['trend_up', 'trend_down', 'ranging']\n",
    "        out['regime'] = np.select(conditions, choices, default='volatile')\n",
    "        \n",
    "        # Regime-based strategy weight adjustments\n",
    "        # In trending markets: weight trend signals more, mean-reversion less\n",
    "        # In ranging markets: weight mean-reversion more, trend signals less\n",
    "        regime_weights = {\n",
    "            'trend_up':   {'trend': 1.5, 'momentum': 1.2, 'reversion': 0.3, 'sentiment': 1.0, 'forecast': 1.0},\n",
    "            'trend_down': {'trend': 1.5, 'momentum': 1.2, 'reversion': 0.3, 'sentiment': 1.0, 'forecast': 1.0},\n",
    "            'ranging':    {'trend': 0.3, 'momentum': 0.5, 'reversion': 1.5, 'sentiment': 1.2, 'forecast': 0.8},\n",
    "            'volatile':   {'trend': 0.5, 'momentum': 0.5, 'reversion': 0.5, 'sentiment': 0.8, 'forecast': 0.5},\n",
    "        }\n",
    "        \n",
    "        for signal_type in ['trend', 'momentum', 'reversion', 'sentiment', 'forecast']:\n",
    "            out[f'w_{signal_type}'] = out['regime'].map(\n",
    "                {k: v[signal_type] for k, v in regime_weights.items()}\n",
    "            ).astype(float)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Classify regimes for NVDA\n",
    "nvda_regime = RegimeDetector.classify(signal_data['NVDA'])\n",
    "\n",
    "regime_counts = nvda_regime['regime'].value_counts()\n",
    "print(\"NVDA Regime Distribution:\")\n",
    "for regime, count in regime_counts.items():\n",
    "    pct = count / len(nvda_regime) * 100\n",
    "    print(f\"  {regime:<15} {count:>5} days ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regimes on price chart\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 12), gridspec_kw={'height_ratios': [3, 1, 1]})\n",
    "\n",
    "recent = nvda_regime.tail(250)\n",
    "\n",
    "# Price with regime coloring\n",
    "ax1 = axes[0]\n",
    "regime_colors = {\n",
    "    'trend_up': 'lime', 'trend_down': 'red',\n",
    "    'ranging': 'yellow', 'volatile': 'gray'\n",
    "}\n",
    "\n",
    "ax1.plot(recent.index, recent['Close'], color='white', linewidth=1, alpha=0.5)\n",
    "for regime, color in regime_colors.items():\n",
    "    mask = recent['regime'] == regime\n",
    "    ax1.fill_between(recent.index, recent['Close'].min() * 0.95, recent['Close'].max() * 1.05,\n",
    "                     where=mask, alpha=0.08, color=color, label=regime)\n",
    "ax1.plot(recent.index, recent['EMA_50'], color='orange', linewidth=1, alpha=0.7, label='EMA 50')\n",
    "ax1.set_ylabel('Price ($)')\n",
    "ax1.set_title('NVDA -- Price with Market Regime Classification')\n",
    "ax1.legend(loc='upper left', fontsize=8)\n",
    "\n",
    "# ADX\n",
    "ax2 = axes[1]\n",
    "ax2.plot(recent.index, recent['ADX'], color='cyan', linewidth=1.5)\n",
    "ax2.axhline(y=25, color='lime', linestyle='--', alpha=0.5, label='Trending (>25)')\n",
    "ax2.axhline(y=20, color='yellow', linestyle='--', alpha=0.5, label='Ranging (<20)')\n",
    "ax2.set_ylabel('ADX')\n",
    "ax2.set_title('ADX (Trend Strength)')\n",
    "ax2.legend(fontsize=8)\n",
    "\n",
    "# Regime-adaptive weights\n",
    "ax3 = axes[2]\n",
    "for wt in ['w_trend', 'w_reversion', 'w_sentiment']:\n",
    "    ax3.plot(recent.index, recent[wt], label=wt.replace('w_', ''), linewidth=1.5, alpha=0.8)\n",
    "ax3.set_ylabel('Weight')\n",
    "ax3.set_title('Regime-Adaptive Signal Weights')\n",
    "ax3.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Green = trending up (favor trend signals)\")\n",
    "print(\"Red = trending down | Yellow = ranging (favor mean-reversion)\")\n",
    "print(\"Gray = volatile (reduce all signals)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Signal Aggregation -- The Composite Score\n",
    "\n",
    "Combine all signals into a single composite score, weighted by:\n",
    "1. **Signal category weight** (from regime detection)\n",
    "2. **Trend strength multiplier** (ADX-based confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each signal to its category for regime-adaptive weighting\n",
    "SIGNAL_CATEGORIES = {\n",
    "    'sig_ema_trend':       ('trend',     1.0),    # (category, base_weight)\n",
    "    'sig_macd':            ('trend',     0.8),\n",
    "    'sig_momentum_5d':     ('momentum',  0.7),\n",
    "    'sig_momentum_20d':    ('momentum',  0.5),\n",
    "    'sig_volume':          ('momentum',  0.6),\n",
    "    'sig_rsi':             ('reversion', 0.8),\n",
    "    'sig_bb':              ('reversion', 0.7),\n",
    "    'sig_mean_reversion':  ('reversion', 0.5),\n",
    "    'sig_sentiment':       ('sentiment', 1.0),\n",
    "    'sig_chronos':         ('forecast',  1.0),\n",
    "}\n",
    "\n",
    "\n",
    "def compute_composite_score(df):\n",
    "    \"\"\"\n",
    "    Compute the regime-adaptive composite signal score.\n",
    "    \n",
    "    For each signal:\n",
    "      weighted_score = signal_value * base_weight * regime_category_weight\n",
    "    \n",
    "    Composite = sum(weighted_scores) / sum(weights)  [normalized to -1, +1]\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    \n",
    "    weighted_sum = pd.Series(0.0, index=out.index)\n",
    "    weight_sum = pd.Series(0.0, index=out.index)\n",
    "    \n",
    "    for signal_name, (category, base_weight) in SIGNAL_CATEGORIES.items():\n",
    "        if signal_name not in out.columns:\n",
    "            continue\n",
    "        \n",
    "        regime_weight = out[f'w_{category}'] if f'w_{category}' in out.columns else 1.0\n",
    "        total_weight = base_weight * regime_weight\n",
    "        \n",
    "        weighted_sum += out[signal_name] * total_weight\n",
    "        weight_sum += total_weight\n",
    "    \n",
    "    out['composite_score'] = (weighted_sum / weight_sum.replace(0, np.nan)).clip(-1, 1)\n",
    "    \n",
    "    # Confidence: how many signals agree on direction?\n",
    "    signal_cols = [s for s in SIGNAL_CATEGORIES.keys() if s in out.columns]\n",
    "    signals_df = out[signal_cols]\n",
    "    out['signal_agreement'] = signals_df.apply(\n",
    "        lambda row: abs(np.sign(row).sum()) / len(row), axis=1\n",
    "    )\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "# Compute composite for all stocks\n",
    "composite_data = {}\n",
    "for ticker in UNIVERSE:\n",
    "    df = build_signal_matrix(all_data[ticker])\n",
    "    df = RegimeDetector.classify(df)\n",
    "    df = compute_composite_score(df)\n",
    "    composite_data[ticker] = df\n",
    "\n",
    "# Show NVDA composite\n",
    "recent = composite_data['NVDA'].tail(10)\n",
    "print(\"NVDA Composite Score (last 10 days):\")\n",
    "print(recent[['Close', 'composite_score', 'signal_agreement', 'regime']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize composite score vs price\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 12), gridspec_kw={'height_ratios': [2, 1, 1]})\n",
    "\n",
    "nvda_comp = composite_data['NVDA'].tail(250)\n",
    "\n",
    "# Price\n",
    "ax1 = axes[0]\n",
    "ax1.plot(nvda_comp.index, nvda_comp['Close'], color='white', linewidth=1.5)\n",
    "ax1.set_ylabel('Price ($)')\n",
    "ax1.set_title('NVDA -- Price vs Composite Signal')\n",
    "\n",
    "# Composite score\n",
    "ax2 = axes[1]\n",
    "colors = ['lime' if s > 0 else 'red' for s in nvda_comp['composite_score']]\n",
    "ax2.bar(nvda_comp.index, nvda_comp['composite_score'], color=colors, alpha=0.7, width=1)\n",
    "ax2.axhline(y=0.3, color='lime', linestyle='--', alpha=0.4, label='Buy threshold')\n",
    "ax2.axhline(y=-0.3, color='red', linestyle='--', alpha=0.4, label='Sell threshold')\n",
    "ax2.axhline(y=0, color='white', alpha=0.2)\n",
    "ax2.set_ylabel('Composite Score')\n",
    "ax2.set_ylim(-1.1, 1.1)\n",
    "ax2.set_title('Composite Signal (-1 = bearish, +1 = bullish)')\n",
    "ax2.legend(fontsize=8)\n",
    "\n",
    "# Signal agreement\n",
    "ax3 = axes[2]\n",
    "ax3.plot(nvda_comp.index, nvda_comp['signal_agreement'], color='cyan', linewidth=1)\n",
    "ax3.axhline(y=0.6, color='yellow', linestyle='--', alpha=0.5, label='High agreement')\n",
    "ax3.set_ylabel('Agreement')\n",
    "ax3.set_title('Signal Agreement (0 = split, 1 = unanimous)')\n",
    "ax3.set_ylim(0, 1.1)\n",
    "ax3.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Trade when composite is strong AND agreement is high.\")\n",
    "print(\"Skip when signals conflict (low agreement).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Portfolio Backtester\n",
    "\n",
    "A portfolio-level backtester that:\n",
    "- Trades multiple stocks simultaneously\n",
    "- Enforces maximum portfolio exposure\n",
    "- Allocates capital across positions\n",
    "- Tracks portfolio-level metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PortfolioPosition:\n",
    "    ticker: str\n",
    "    direction: str\n",
    "    entry_date: pd.Timestamp\n",
    "    entry_price: float\n",
    "    shares: int\n",
    "    stop_loss: float\n",
    "    take_profit: Optional[float]\n",
    "    bars_held: int = 0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PortfolioTrade:\n",
    "    ticker: str\n",
    "    direction: str\n",
    "    entry_date: pd.Timestamp\n",
    "    exit_date: pd.Timestamp\n",
    "    entry_price: float\n",
    "    exit_price: float\n",
    "    shares: int\n",
    "    pnl: float\n",
    "    pnl_pct: float\n",
    "    exit_reason: str\n",
    "    composite_score: float\n",
    "    regime: str\n",
    "\n",
    "\n",
    "class PortfolioBacktester:\n",
    "    \"\"\"\n",
    "    Multi-stock portfolio backtester with the composite signal system.\n",
    "    \n",
    "    Rules:\n",
    "    - Max positions: configurable (default 5)\n",
    "    - Max per-stock allocation: 25% of capital\n",
    "    - Entry: composite > threshold AND agreement > min_agreement\n",
    "    - Exit: opposite composite threshold OR stop loss OR take profit\n",
    "    - Position sizing: ATR-based risk per trade\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_capital=100000, max_positions=5,\n",
    "                 max_per_stock_pct=25, risk_per_trade_pct=1.0,\n",
    "                 entry_threshold=0.25, exit_threshold=-0.1,\n",
    "                 min_agreement=0.4, atr_stop_mult=2.0, atr_tp_mult=4.0,\n",
    "                 slippage_pct=0.0005):\n",
    "        self.initial_capital = initial_capital\n",
    "        self.max_positions = max_positions\n",
    "        self.max_per_stock_pct = max_per_stock_pct\n",
    "        self.risk_per_trade_pct = risk_per_trade_pct\n",
    "        self.entry_threshold = entry_threshold\n",
    "        self.exit_threshold = exit_threshold\n",
    "        self.min_agreement = min_agreement\n",
    "        self.atr_stop_mult = atr_stop_mult\n",
    "        self.atr_tp_mult = atr_tp_mult\n",
    "        self.slippage_pct = slippage_pct\n",
    "    \n",
    "    def run(self, composite_data_dict):\n",
    "        \"\"\"\n",
    "        Run portfolio backtest across multiple stocks.\n",
    "        \n",
    "        Parameters:\n",
    "            composite_data_dict: {ticker: DataFrame with composite scores}\n",
    "        \"\"\"\n",
    "        # Align all DataFrames to common dates\n",
    "        common_dates = None\n",
    "        for ticker, df in composite_data_dict.items():\n",
    "            if common_dates is None:\n",
    "                common_dates = set(df.index)\n",
    "            else:\n",
    "                common_dates = common_dates & set(df.index)\n",
    "        common_dates = sorted(common_dates)\n",
    "        \n",
    "        capital = self.initial_capital\n",
    "        positions = {}  # ticker -> PortfolioPosition\n",
    "        trades = []\n",
    "        equity_curve = []\n",
    "        \n",
    "        for i, date in enumerate(common_dates[1:], 1):\n",
    "            prev_date = common_dates[i - 1]\n",
    "            \n",
    "            # Calculate unrealized P&L\n",
    "            unrealized = 0\n",
    "            for ticker, pos in positions.items():\n",
    "                current_price = composite_data_dict[ticker].loc[date, 'Close']\n",
    "                if pos.direction == 'long':\n",
    "                    unrealized += (current_price - pos.entry_price) * pos.shares\n",
    "                pos.bars_held += 1\n",
    "            \n",
    "            equity_curve.append({\n",
    "                'date': date,\n",
    "                'equity': capital + unrealized,\n",
    "                'capital': capital,\n",
    "                'n_positions': len(positions),\n",
    "            })\n",
    "            \n",
    "            # --- Check exits ---\n",
    "            tickers_to_close = []\n",
    "            for ticker, pos in positions.items():\n",
    "                df = composite_data_dict[ticker]\n",
    "                if date not in df.index:\n",
    "                    continue\n",
    "                row = df.loc[date]\n",
    "                \n",
    "                exit_price = None\n",
    "                exit_reason = None\n",
    "                \n",
    "                # Stop loss\n",
    "                if pos.direction == 'long' and row['Low'] <= pos.stop_loss:\n",
    "                    exit_price = pos.stop_loss\n",
    "                    exit_reason = 'stop_loss'\n",
    "                # Take profit\n",
    "                elif pos.direction == 'long' and pos.take_profit and row['High'] >= pos.take_profit:\n",
    "                    exit_price = pos.take_profit\n",
    "                    exit_reason = 'take_profit'\n",
    "                # Signal reversal\n",
    "                elif row['composite_score'] < self.exit_threshold:\n",
    "                    exit_price = row['Open'] * (1 - self.slippage_pct)\n",
    "                    exit_reason = 'signal_exit'\n",
    "                # Max holding period (20 bars)\n",
    "                elif pos.bars_held >= 20:\n",
    "                    exit_price = row['Open'] * (1 - self.slippage_pct)\n",
    "                    exit_reason = 'max_hold'\n",
    "                \n",
    "                if exit_price:\n",
    "                    pnl = (exit_price - pos.entry_price) * pos.shares\n",
    "                    pnl_pct = (exit_price / pos.entry_price - 1) * 100\n",
    "                    capital += pnl\n",
    "                    trades.append(PortfolioTrade(\n",
    "                        ticker=ticker, direction=pos.direction,\n",
    "                        entry_date=pos.entry_date, exit_date=date,\n",
    "                        entry_price=pos.entry_price, exit_price=exit_price,\n",
    "                        shares=pos.shares, pnl=pnl, pnl_pct=pnl_pct,\n",
    "                        exit_reason=exit_reason,\n",
    "                        composite_score=row['composite_score'],\n",
    "                        regime=row.get('regime', 'unknown'),\n",
    "                    ))\n",
    "                    tickers_to_close.append(ticker)\n",
    "            \n",
    "            for ticker in tickers_to_close:\n",
    "                del positions[ticker]\n",
    "            \n",
    "            # --- Check entries ---\n",
    "            if len(positions) < self.max_positions:\n",
    "                # Rank all stocks by composite score\n",
    "                candidates = []\n",
    "                for ticker in composite_data_dict:\n",
    "                    if ticker in positions:\n",
    "                        continue\n",
    "                    df = composite_data_dict[ticker]\n",
    "                    if prev_date not in df.index:\n",
    "                        continue\n",
    "                    prev_row = df.loc[prev_date]\n",
    "                    \n",
    "                    if (prev_row['composite_score'] > self.entry_threshold and\n",
    "                        prev_row['signal_agreement'] > self.min_agreement):\n",
    "                        candidates.append((ticker, prev_row['composite_score']))\n",
    "                \n",
    "                # Take the top candidates by composite score\n",
    "                candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "                slots = self.max_positions - len(positions)\n",
    "                \n",
    "                for ticker, score in candidates[:slots]:\n",
    "                    df = composite_data_dict[ticker]\n",
    "                    row = df.loc[date]\n",
    "                    prev_row = df.loc[prev_date]\n",
    "                    \n",
    "                    entry_price = row['Open'] * (1 + self.slippage_pct)\n",
    "                    atr = prev_row['ATR']\n",
    "                    stop_loss = entry_price - self.atr_stop_mult * atr\n",
    "                    take_profit = entry_price + self.atr_tp_mult * atr\n",
    "                    \n",
    "                    # Position sizing\n",
    "                    risk_amount = capital * (self.risk_per_trade_pct / 100)\n",
    "                    risk_per_share = entry_price - stop_loss\n",
    "                    if risk_per_share <= 0:\n",
    "                        continue\n",
    "                    \n",
    "                    shares = max(1, int(risk_amount / risk_per_share))\n",
    "                    max_alloc = capital * (self.max_per_stock_pct / 100)\n",
    "                    shares = min(shares, max(1, int(max_alloc / entry_price)))\n",
    "                    \n",
    "                    if shares * entry_price > capital * 0.95:\n",
    "                        continue\n",
    "                    \n",
    "                    positions[ticker] = PortfolioPosition(\n",
    "                        ticker=ticker, direction='long',\n",
    "                        entry_date=date, entry_price=entry_price,\n",
    "                        shares=shares, stop_loss=stop_loss,\n",
    "                        take_profit=take_profit,\n",
    "                    )\n",
    "        \n",
    "        # Close remaining positions\n",
    "        for ticker, pos in positions.items():\n",
    "            df = composite_data_dict[ticker]\n",
    "            last_date = common_dates[-1]\n",
    "            exit_price = df.loc[last_date, 'Close']\n",
    "            pnl = (exit_price - pos.entry_price) * pos.shares\n",
    "            pnl_pct = (exit_price / pos.entry_price - 1) * 100\n",
    "            capital += pnl\n",
    "            trades.append(PortfolioTrade(\n",
    "                ticker=ticker, direction=pos.direction,\n",
    "                entry_date=pos.entry_date, exit_date=last_date,\n",
    "                entry_price=pos.entry_price, exit_price=exit_price,\n",
    "                shares=pos.shares, pnl=pnl, pnl_pct=pnl_pct,\n",
    "                exit_reason='end_of_test', composite_score=0, regime='unknown',\n",
    "            ))\n",
    "        \n",
    "        equity_df = pd.DataFrame(equity_curve).set_index('date')\n",
    "        metrics = self._compute_metrics(trades, equity_df)\n",
    "        \n",
    "        return {'trades': trades, 'equity': equity_df, 'metrics': metrics,\n",
    "                'final_capital': capital}\n",
    "    \n",
    "    def _compute_metrics(self, trades, equity_df):\n",
    "        if not trades:\n",
    "            return {'total_trades': 0}\n",
    "        \n",
    "        pnls = [t.pnl for t in trades]\n",
    "        winners = [p for p in pnls if p > 0]\n",
    "        losers = [p for p in pnls if p <= 0]\n",
    "        \n",
    "        equity = equity_df['equity']\n",
    "        returns = equity.pct_change().dropna()\n",
    "        peak = equity.expanding().max()\n",
    "        drawdown = (equity - peak) / peak\n",
    "        \n",
    "        sharpe = (returns.mean() / returns.std() * np.sqrt(252)) if returns.std() > 0 else 0\n",
    "        gross_profit = sum(winners) if winners else 0\n",
    "        gross_loss = abs(sum(losers)) if losers else 1\n",
    "        \n",
    "        return {\n",
    "            'total_trades': len(trades),\n",
    "            'winners': len(winners),\n",
    "            'losers': len(losers),\n",
    "            'win_rate': len(winners) / len(trades) * 100,\n",
    "            'total_pnl': sum(pnls),\n",
    "            'total_return_pct': (equity.iloc[-1] / self.initial_capital - 1) * 100,\n",
    "            'avg_trade_pnl': np.mean(pnls),\n",
    "            'avg_win': np.mean(winners) if winners else 0,\n",
    "            'avg_loss': np.mean(losers) if losers else 0,\n",
    "            'profit_factor': gross_profit / gross_loss if gross_loss > 0 else float('inf'),\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'max_drawdown_pct': drawdown.min() * 100,\n",
    "            'avg_bars_held': np.mean([t.exit_date - t.entry_date for t in trades]).days,\n",
    "            'tickers_traded': len(set(t.ticker for t in trades)),\n",
    "            'exit_reasons': pd.Series([t.exit_reason for t in trades]).value_counts().to_dict(),\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"PortfolioBacktester ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the portfolio backtest\n",
    "print(\"Running portfolio backtest across all stocks...\\n\")\n",
    "\n",
    "backtester = PortfolioBacktester(\n",
    "    initial_capital=100000,\n",
    "    max_positions=5,\n",
    "    risk_per_trade_pct=1.0,\n",
    "    entry_threshold=0.25,\n",
    "    exit_threshold=-0.1,\n",
    "    min_agreement=0.4,\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "results = backtester.run(composite_data)\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "m = results['metrics']\n",
    "print(f\"Backtest completed in {elapsed:.1f}s\\n\")\n",
    "print(f\"{'='*55}\")\n",
    "print(f\"  PORTFOLIO BACKTEST RESULTS\")\n",
    "print(f\"{'='*55}\")\n",
    "print(f\"  Total Trades:     {m['total_trades']}\")\n",
    "print(f\"  Win / Lose:       {m['winners']} / {m['losers']}\")\n",
    "print(f\"  Win Rate:         {m['win_rate']:.1f}%\")\n",
    "print(f\"  Avg Win:          ${m['avg_win']:>+10,.2f}\")\n",
    "print(f\"  Avg Loss:         ${m['avg_loss']:>+10,.2f}\")\n",
    "print(f\"  ---\")\n",
    "print(f\"  Total P&L:        ${m['total_pnl']:>+10,.2f}\")\n",
    "print(f\"  Total Return:     {m['total_return_pct']:>+10.2f}%\")\n",
    "print(f\"  Profit Factor:    {m['profit_factor']:>10.2f}\")\n",
    "print(f\"  Sharpe Ratio:     {m['sharpe_ratio']:>10.2f}\")\n",
    "print(f\"  Max Drawdown:     {m['max_drawdown_pct']:>10.2f}%\")\n",
    "print(f\"  ---\")\n",
    "print(f\"  Avg Hold (days):  {m['avg_bars_held']}\")\n",
    "print(f\"  Stocks Traded:    {m['tickers_traded']}\")\n",
    "print(f\"  Exit Reasons:     {m['exit_reasons']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize portfolio results\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "gs = gridspec.GridSpec(4, 2, hspace=0.35, wspace=0.3)\n",
    "\n",
    "equity_df = results['equity']\n",
    "trades = results['trades']\n",
    "\n",
    "# 1. Equity Curve\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1.plot(equity_df.index, equity_df['equity'], color='cyan', linewidth=1.5)\n",
    "ax1.axhline(y=100000, color='yellow', linestyle='--', alpha=0.3, label='Starting Capital')\n",
    "ax1.set_ylabel('Equity ($)')\n",
    "ax1.set_title(f'Portfolio Equity Curve (Return: {m[\"total_return_pct\"]:+.1f}%, Sharpe: {m[\"sharpe_ratio\"]:.2f})')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Drawdown\n",
    "ax2 = fig.add_subplot(gs[1, :])\n",
    "peak = equity_df['equity'].expanding().max()\n",
    "dd = (equity_df['equity'] - peak) / peak * 100\n",
    "ax2.fill_between(dd.index, dd.values, 0, color='red', alpha=0.4)\n",
    "ax2.set_ylabel('Drawdown (%)')\n",
    "ax2.set_title(f'Portfolio Drawdown (Max: {m[\"max_drawdown_pct\"]:.1f}%)')\n",
    "\n",
    "# 3. Trades by stock\n",
    "ax3 = fig.add_subplot(gs[2, 0])\n",
    "trade_df = pd.DataFrame([{'ticker': t.ticker, 'pnl': t.pnl} for t in trades])\n",
    "if len(trade_df) > 0:\n",
    "    by_ticker = trade_df.groupby('ticker')['pnl'].sum().sort_values()\n",
    "    colors = ['lime' if v > 0 else 'red' for v in by_ticker.values]\n",
    "    ax3.barh(by_ticker.index, by_ticker.values, color=colors, alpha=0.8)\n",
    "    ax3.axvline(x=0, color='white', alpha=0.3)\n",
    "ax3.set_xlabel('Total P&L ($)')\n",
    "ax3.set_title('P&L by Stock')\n",
    "\n",
    "# 4. P&L distribution\n",
    "ax4 = fig.add_subplot(gs[2, 1])\n",
    "pnls = [t.pnl for t in trades]\n",
    "ax4.hist(pnls, bins=30, color='cyan', alpha=0.7, edgecolor='white', linewidth=0.5)\n",
    "ax4.axvline(x=0, color='yellow', linestyle='--', alpha=0.5)\n",
    "ax4.axvline(x=np.mean(pnls), color='lime', linestyle='--', alpha=0.7, label=f'Mean: ${np.mean(pnls):+,.0f}')\n",
    "ax4.set_xlabel('Trade P&L ($)')\n",
    "ax4.set_title('Trade P&L Distribution')\n",
    "ax4.legend()\n",
    "\n",
    "# 5. Number of positions over time\n",
    "ax5 = fig.add_subplot(gs[3, 0])\n",
    "ax5.fill_between(equity_df.index, equity_df['n_positions'], color='cyan', alpha=0.4)\n",
    "ax5.set_ylabel('# Positions')\n",
    "ax5.set_title('Active Positions Over Time')\n",
    "ax5.set_ylim(0, backtester.max_positions + 1)\n",
    "\n",
    "# 6. Win rate by exit reason\n",
    "ax6 = fig.add_subplot(gs[3, 1])\n",
    "if len(trades) > 0:\n",
    "    reason_df = pd.DataFrame([{'reason': t.exit_reason, 'win': t.pnl > 0} for t in trades])\n",
    "    reason_stats = reason_df.groupby('reason')['win'].agg(['mean', 'count'])\n",
    "    reason_stats.columns = ['win_rate', 'count']\n",
    "    reason_stats['win_rate'] *= 100\n",
    "    bars = ax6.bar(range(len(reason_stats)), reason_stats['win_rate'], color='cyan', alpha=0.8)\n",
    "    ax6.set_xticks(range(len(reason_stats)))\n",
    "    ax6.set_xticklabels(reason_stats.index, rotation=30, ha='right', fontsize=8)\n",
    "    ax6.axhline(y=50, color='yellow', linestyle='--', alpha=0.5)\n",
    "    ax6.set_ylabel('Win Rate (%)')\n",
    "    ax6.set_title('Win Rate by Exit Reason')\n",
    "    for bar, (_, row) in zip(bars, reason_stats.iterrows()):\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'n={int(row[\"count\"])}', ha='center', fontsize=8, color='white')\n",
    "\n",
    "plt.suptitle('Multi-Signal Portfolio Backtest', fontsize=14, y=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Signal Correlation & Attribution\n",
    "\n",
    "Which signals are redundant? Which add unique value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal correlation matrix\n",
    "sig_cols = sorted([c for c in composite_data['NVDA'].columns if c.startswith('sig_')])\n",
    "\n",
    "# Stack all stocks for a more robust correlation estimate\n",
    "all_signals = pd.concat([\n",
    "    composite_data[t][sig_cols] for t in UNIVERSE\n",
    "], ignore_index=True)\n",
    "\n",
    "corr_matrix = all_signals.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "labels = [c.replace('sig_', '') for c in sig_cols]\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "            xticklabels=labels, yticklabels=labels, ax=ax, linewidths=0.5,\n",
    "            vmin=-1, vmax=1)\n",
    "ax.set_title('Signal Correlation Matrix (Across All Stocks)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "print(\"Highly Correlated Pairs (|r| > 0.5):\")\n",
    "for i in range(len(sig_cols)):\n",
    "    for j in range(i+1, len(sig_cols)):\n",
    "        r = corr_matrix.iloc[i, j]\n",
    "        if abs(r) > 0.5:\n",
    "            print(f\"  {sig_cols[i]:<25} <-> {sig_cols[j]:<25} r = {r:+.3f}\")\n",
    "\n",
    "print(\"\\nHighly correlated signals are partially redundant.\")\n",
    "print(\"Low correlation between sources = better diversification of signals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal attribution: how much does each signal predict next-day returns?\n",
    "print(\"Signal Predictive Power (correlation with next-day return):\\n\")\n",
    "\n",
    "attribution_results = []\n",
    "for ticker in UNIVERSE:\n",
    "    df = composite_data[ticker].copy()\n",
    "    df['next_return'] = df['Close'].pct_change().shift(-1)\n",
    "    \n",
    "    for col in sig_cols:\n",
    "        corr = df[col].corr(df['next_return'])\n",
    "        attribution_results.append({\n",
    "            'signal': col.replace('sig_', ''),\n",
    "            'ticker': ticker,\n",
    "            'correlation': corr,\n",
    "        })\n",
    "\n",
    "attr_df = pd.DataFrame(attribution_results)\n",
    "avg_attr = attr_df.groupby('signal')['correlation'].agg(['mean', 'std']).round(4)\n",
    "avg_attr = avg_attr.sort_values('mean', ascending=False)\n",
    "\n",
    "print(f\"{'Signal':<25} {'Avg Corr':>10} {'Std':>10} {'Consistent?':>12}\")\n",
    "print(\"-\" * 60)\n",
    "for signal, row in avg_attr.iterrows():\n",
    "    consistent = 'Yes' if abs(row['mean']) > row['std'] else 'No'\n",
    "    print(f\"{signal:<25} {row['mean']:>+10.4f} {row['std']:>10.4f} {consistent:>12}\")\n",
    "\n",
    "print(\"\\nPositive correlation = signal correctly predicts direction.\")\n",
    "print(\"Consistent = signal effect is stable across stocks (good).\")\n",
    "print(\"\\nNote: Even small positive correlations (0.01-0.05) can be profitable\")\n",
    "print(\"when combined with proper risk management and multiple signals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Monte Carlo Simulation\n",
    "\n",
    "Stress-test the strategy by randomly resampling trades.\n",
    "This shows the range of outcomes you might expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_simulation(trades, initial_capital=100000, n_simulations=1000, n_trades=None):\n",
    "    \"\"\"\n",
    "    Bootstrap Monte Carlo: resample from actual trades to simulate\n",
    "    alternative histories.\n",
    "    \"\"\"\n",
    "    if not trades:\n",
    "        return None\n",
    "    \n",
    "    pnls = np.array([t.pnl for t in trades])\n",
    "    if n_trades is None:\n",
    "        n_trades = len(pnls)\n",
    "    \n",
    "    results = []\n",
    "    for _ in range(n_simulations):\n",
    "        # Randomly sample trades with replacement\n",
    "        sampled_pnls = np.random.choice(pnls, size=n_trades, replace=True)\n",
    "        equity = initial_capital + np.cumsum(sampled_pnls)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_return = (equity[-1] / initial_capital - 1) * 100\n",
    "        peak = np.maximum.accumulate(np.concatenate([[initial_capital], equity]))\n",
    "        dd = (np.concatenate([[initial_capital], equity]) - peak) / peak\n",
    "        max_dd = dd.min() * 100\n",
    "        \n",
    "        results.append({\n",
    "            'final_equity': equity[-1],\n",
    "            'total_return': total_return,\n",
    "            'max_drawdown': max_dd,\n",
    "            'equity_curve': equity,\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Run Monte Carlo\n",
    "print(\"Running 1,000 Monte Carlo simulations...\\n\")\n",
    "mc_results = monte_carlo_simulation(results['trades'], n_simulations=1000)\n",
    "\n",
    "if mc_results:\n",
    "    returns = [r['total_return'] for r in mc_results]\n",
    "    drawdowns = [r['max_drawdown'] for r in mc_results]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # Equity curve fan\n",
    "    ax1 = axes[0, 0]\n",
    "    for r in mc_results[:100]:  # Plot 100 paths\n",
    "        ax1.plot(r['equity_curve'], alpha=0.03, color='cyan', linewidth=0.5)\n",
    "    median_equity = np.median([r['equity_curve'] for r in mc_results], axis=0)\n",
    "    p5 = np.percentile([r['equity_curve'] for r in mc_results], 5, axis=0)\n",
    "    p95 = np.percentile([r['equity_curve'] for r in mc_results], 95, axis=0)\n",
    "    ax1.plot(median_equity, color='yellow', linewidth=2, label='Median')\n",
    "    ax1.plot(p5, color='red', linestyle='--', linewidth=1, label='5th percentile')\n",
    "    ax1.plot(p95, color='lime', linestyle='--', linewidth=1, label='95th percentile')\n",
    "    ax1.axhline(y=100000, color='white', alpha=0.2)\n",
    "    ax1.set_title('Monte Carlo Equity Paths')\n",
    "    ax1.set_ylabel('Equity ($)')\n",
    "    ax1.legend(fontsize=8)\n",
    "    \n",
    "    # Return distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(returns, bins=50, color='cyan', alpha=0.7, edgecolor='white', linewidth=0.5)\n",
    "    ax2.axvline(x=0, color='yellow', linestyle='--', alpha=0.5)\n",
    "    ax2.axvline(x=np.median(returns), color='lime', linestyle='--', label=f'Median: {np.median(returns):+.1f}%')\n",
    "    ax2.set_title('Return Distribution (1000 simulations)')\n",
    "    ax2.set_xlabel('Total Return (%)')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Drawdown distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.hist(drawdowns, bins=50, color='red', alpha=0.6, edgecolor='white', linewidth=0.5)\n",
    "    ax3.axvline(x=np.median(drawdowns), color='yellow', linestyle='--', label=f'Median: {np.median(drawdowns):.1f}%')\n",
    "    ax3.set_title('Max Drawdown Distribution')\n",
    "    ax3.set_xlabel('Max Drawdown (%)')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Summary stats\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    prob_profit = sum(1 for r in returns if r > 0) / len(returns) * 100\n",
    "    summary = (\n",
    "        f\"Monte Carlo Summary (n=1000)\\n\"\n",
    "        f\"{'\u2500'*35}\\n\"\n",
    "        f\"P(Profit):      {prob_profit:.0f}%\\n\\n\"\n",
    "        f\"Return Percentiles:\\n\"\n",
    "        f\"  5th:    {np.percentile(returns, 5):+.1f}%\\n\"\n",
    "        f\"  25th:   {np.percentile(returns, 25):+.1f}%\\n\"\n",
    "        f\"  Median: {np.median(returns):+.1f}%\\n\"\n",
    "        f\"  75th:   {np.percentile(returns, 75):+.1f}%\\n\"\n",
    "        f\"  95th:   {np.percentile(returns, 95):+.1f}%\\n\\n\"\n",
    "        f\"Max Drawdown Percentiles:\\n\"\n",
    "        f\"  Median: {np.median(drawdowns):.1f}%\\n\"\n",
    "        f\"  Worst 5%: {np.percentile(drawdowns, 5):.1f}%\\n\"\n",
    "    )\n",
    "    ax4.text(0.1, 0.9, summary, transform=ax4.transAxes, fontsize=11,\n",
    "             verticalalignment='top', fontfamily='monospace', color='white')\n",
    "    \n",
    "    plt.suptitle('Monte Carlo Robustness Analysis', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Probability of profit: {prob_profit:.0f}%\")\n",
    "    print(f\"Expected return: {np.median(returns):+.1f}% (median)\")\n",
    "    print(f\"Worst case (5th pctile): {np.percentile(returns, 5):+.1f}%\")\n",
    "    print(f\"Best case (95th pctile): {np.percentile(returns, 95):+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Walk-Forward Validation\n",
    "\n",
    "The ultimate test: does this system work on unseen data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_portfolio(all_raw_data, tickers, train_days=252, test_days=63, step_days=63):\n",
    "    \"\"\"\n",
    "    Walk-forward analysis for the full portfolio system.\n",
    "    \"\"\"\n",
    "    # Use the longest common date range\n",
    "    common_dates = None\n",
    "    for ticker in tickers:\n",
    "        dates = set(all_raw_data[ticker].index)\n",
    "        common_dates = dates if common_dates is None else common_dates & dates\n",
    "    common_dates = sorted(common_dates)\n",
    "    total_days = len(common_dates)\n",
    "    \n",
    "    window_results = []\n",
    "    start = 0\n",
    "    \n",
    "    while start + train_days + test_days <= total_days:\n",
    "        test_start = start + train_days\n",
    "        test_end = test_start + test_days\n",
    "        \n",
    "        test_start_date = common_dates[test_start]\n",
    "        test_end_date = common_dates[min(test_end, total_days) - 1]\n",
    "        \n",
    "        # Build signals using train+test data (indicators need history)\n",
    "        # But only evaluate on test period\n",
    "        composite_test = {}\n",
    "        for ticker in tickers:\n",
    "            raw = all_raw_data[ticker]\n",
    "            subset = raw[(raw.index >= common_dates[start]) & (raw.index <= test_end_date)]\n",
    "            if len(subset) < train_days + 10:\n",
    "                continue\n",
    "            \n",
    "            full_signals = build_signal_matrix(subset)\n",
    "            full_signals = RegimeDetector.classify(full_signals)\n",
    "            full_signals = compute_composite_score(full_signals)\n",
    "            \n",
    "            # Only keep test period\n",
    "            oos = full_signals[full_signals.index >= test_start_date]\n",
    "            if len(oos) > 5:\n",
    "                composite_test[ticker] = oos\n",
    "        \n",
    "        if len(composite_test) < 3:\n",
    "            start += step_days\n",
    "            continue\n",
    "        \n",
    "        # Run backtest on test period\n",
    "        bt = PortfolioBacktester(initial_capital=100000, max_positions=5)\n",
    "        result = bt.run(composite_test)\n",
    "        m = result['metrics']\n",
    "        \n",
    "        window_results.append({\n",
    "            'test_start': test_start_date.strftime('%Y-%m-%d'),\n",
    "            'test_end': test_end_date.strftime('%Y-%m-%d'),\n",
    "            'trades': m.get('total_trades', 0),\n",
    "            'return_pct': m.get('total_return_pct', 0),\n",
    "            'sharpe': m.get('sharpe_ratio', 0),\n",
    "            'win_rate': m.get('win_rate', 0),\n",
    "            'max_dd': m.get('max_drawdown_pct', 0),\n",
    "            'profit_factor': m.get('profit_factor', 0),\n",
    "        })\n",
    "        \n",
    "        start += step_days\n",
    "    \n",
    "    return pd.DataFrame(window_results)\n",
    "\n",
    "\n",
    "# Run walk-forward\n",
    "print(\"Running walk-forward analysis on full portfolio system...\\n\")\n",
    "wf_df = walk_forward_portfolio(all_data, UNIVERSE, train_days=200, test_days=60, step_days=60)\n",
    "\n",
    "if len(wf_df) > 0:\n",
    "    print(\"Out-of-Sample Results by Window:\\n\")\n",
    "    print(wf_df.round(2).to_string(index=False))\n",
    "    \n",
    "    profitable = (wf_df['return_pct'] > 0).sum()\n",
    "    total = len(wf_df)\n",
    "    print(f\"\\n--- Walk-Forward Summary ---\")\n",
    "    print(f\"  Windows: {total}\")\n",
    "    print(f\"  Profitable: {profitable}/{total} ({profitable/total*100:.0f}%)\")\n",
    "    print(f\"  Avg OOS Return: {wf_df['return_pct'].mean():+.2f}%\")\n",
    "    print(f\"  Avg OOS Sharpe: {wf_df['sharpe'].mean():.2f}\")\n",
    "    print(f\"  Avg OOS Max DD: {wf_df['max_dd'].mean():.1f}%\")\n",
    "else:\n",
    "    print(\"Not enough data for walk-forward with these window sizes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize walk-forward results\n",
    "if len(wf_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Return per window\n",
    "    colors = ['lime' if r > 0 else 'red' for r in wf_df['return_pct']]\n",
    "    axes[0, 0].bar(range(len(wf_df)), wf_df['return_pct'], color=colors, alpha=0.8)\n",
    "    axes[0, 0].axhline(y=0, color='white', alpha=0.3)\n",
    "    axes[0, 0].set_title('OOS Return per Window')\n",
    "    axes[0, 0].set_xlabel('Window')\n",
    "    axes[0, 0].set_ylabel('Return (%)')\n",
    "    \n",
    "    # Cumulative return\n",
    "    cum_ret = (1 + wf_df['return_pct']/100).cumprod() * 100 - 100\n",
    "    axes[0, 1].plot(range(len(cum_ret)), cum_ret, color='cyan', marker='o', linewidth=2)\n",
    "    axes[0, 1].axhline(y=0, color='yellow', linestyle='--', alpha=0.3)\n",
    "    axes[0, 1].set_title('Cumulative OOS Return')\n",
    "    axes[0, 1].set_xlabel('Window')\n",
    "    axes[0, 1].set_ylabel('Cumulative Return (%)')\n",
    "    \n",
    "    # Sharpe\n",
    "    axes[1, 0].bar(range(len(wf_df)), wf_df['sharpe'], color='orange', alpha=0.8)\n",
    "    axes[1, 0].axhline(y=0, color='white', alpha=0.3)\n",
    "    axes[1, 0].set_title('OOS Sharpe per Window')\n",
    "    axes[1, 0].set_xlabel('Window')\n",
    "    axes[1, 0].set_ylabel('Sharpe')\n",
    "    \n",
    "    # Profit Factor\n",
    "    pf = wf_df['profit_factor'].clip(0, 5)  # cap for display\n",
    "    pf_colors = ['lime' if p > 1 else 'red' for p in pf]\n",
    "    axes[1, 1].bar(range(len(pf)), pf, color=pf_colors, alpha=0.8)\n",
    "    axes[1, 1].axhline(y=1, color='yellow', linestyle='--', alpha=0.5, label='Breakeven')\n",
    "    axes[1, 1].set_title('OOS Profit Factor per Window')\n",
    "    axes[1, 1].set_xlabel('Window')\n",
    "    axes[1, 1].set_ylabel('Profit Factor')\n",
    "    axes[1, 1].legend(fontsize=8)\n",
    "    \n",
    "    plt.suptitle('Walk-Forward Validation: Multi-Signal Portfolio', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Parameter Sensitivity & Optimization\n",
    "\n",
    "How sensitive is the portfolio system to key parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep entry threshold and agreement threshold\n",
    "entry_thresholds = [0.10, 0.15, 0.20, 0.25, 0.30, 0.40]\n",
    "agreement_thresholds = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "\n",
    "param_results = []\n",
    "\n",
    "print(\"Running parameter sensitivity sweep...\\n\")\n",
    "for entry_t in entry_thresholds:\n",
    "    for agree_t in agreement_thresholds:\n",
    "        bt = PortfolioBacktester(\n",
    "            initial_capital=100000,\n",
    "            entry_threshold=entry_t,\n",
    "            min_agreement=agree_t,\n",
    "        )\n",
    "        result = bt.run(composite_data)\n",
    "        m = result['metrics']\n",
    "        param_results.append({\n",
    "            'Entry Threshold': entry_t,\n",
    "            'Min Agreement': agree_t,\n",
    "            'Trades': m.get('total_trades', 0),\n",
    "            'Return %': m.get('total_return_pct', 0),\n",
    "            'Sharpe': m.get('sharpe_ratio', 0),\n",
    "            'Max DD %': m.get('max_drawdown_pct', 0),\n",
    "            'Win Rate': m.get('win_rate', 0),\n",
    "        })\n",
    "\n",
    "param_sweep = pd.DataFrame(param_results)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sharpe heatmap\n",
    "pivot_sharpe = param_sweep.pivot(index='Entry Threshold', columns='Min Agreement', values='Sharpe')\n",
    "sns.heatmap(pivot_sharpe, annot=True, fmt='.2f', cmap='RdYlGn', center=0,\n",
    "            ax=axes[0], linewidths=0.5)\n",
    "axes[0].set_title('Sharpe Ratio by Parameters')\n",
    "\n",
    "# Return heatmap\n",
    "pivot_ret = param_sweep.pivot(index='Entry Threshold', columns='Min Agreement', values='Return %')\n",
    "sns.heatmap(pivot_ret, annot=True, fmt='.1f', cmap='RdYlGn', center=0,\n",
    "            ax=axes[1], linewidths=0.5)\n",
    "axes[1].set_title('Total Return (%) by Parameters')\n",
    "\n",
    "plt.suptitle('Parameter Sensitivity Analysis', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "positive = (param_sweep['Return %'] > 0).sum()\n",
    "total = len(param_sweep)\n",
    "print(f\"Profitable configurations: {positive}/{total} ({positive/total*100:.0f}%)\")\n",
    "best = param_sweep.sort_values('Sharpe', ascending=False).iloc[0]\n",
    "print(f\"Best Sharpe config: Entry={best['Entry Threshold']}, Agreement={best['Min Agreement']}\")\n",
    "print(f\"  -> Sharpe: {best['Sharpe']:.2f}, Return: {best['Return %']:+.1f}%, Trades: {int(best['Trades'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Summary & Architecture\n",
    "\n",
    "### What We Built\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| **TechnicalSignals** | EMA, RSI, MACD, BB -> normalized -1 to +1 signals |\n",
    "| **MomentumSignals** | Multi-horizon momentum + volume + mean reversion |\n",
    "| **SyntheticSentimentSignals** | Simulated sentiment & Chronos (replace with live data) |\n",
    "| **RegimeDetector** | ADX-based trend/range/volatile classification |\n",
    "| **Composite Score** | Regime-adaptive weighted average of all 10 signals |\n",
    "| **PortfolioBacktester** | Multi-stock simultaneous trading with capital management |\n",
    "| **Signal Correlation** | Identify redundant vs complementary signals |\n",
    "| **Signal Attribution** | Measure each signal's predictive power |\n",
    "| **Monte Carlo** | Bootstrap stress testing with probability of profit |\n",
    "| **Walk-Forward** | Out-of-sample validation across rolling windows |\n",
    "| **Parameter Sensitivity** | Robustness across parameter ranges |\n",
    "\n",
    "### Key Lessons\n",
    "\n",
    "1. **Combining signals beats any single signal** -- diversification of information sources\n",
    "2. **Regime adaptation matters** -- trend-following fails in ranging markets and vice versa\n",
    "3. **Signal agreement is a confidence filter** -- high agreement = higher conviction\n",
    "4. **Monte Carlo reveals the range of possible outcomes** -- not just the backtest result\n",
    "5. **Walk-forward is the only honest validation** -- single backtests overstate performance\n",
    "6. **Robustness > optimization** -- the best config is one that works across many settings\n",
    "\n",
    "### Production Integration\n",
    "\n",
    "To use this live, replace the synthetic signals with:\n",
    "- **`sig_sentiment`**: Real-time output from Notebook 02's FinBERT pipeline\n",
    "- **`sig_chronos`**: Live Chronos forecasts from Notebook 04\n",
    "- Add LLM signals from Notebook 05 as an additional signal category\n",
    "\n",
    "### Coming Up Next\n",
    "\n",
    "**Notebook 07: Paper Trading Bot** -- Connect to the Alpaca API to paper trade\n",
    "this combined system in real-time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DayTrader (Python 3.10)",
   "language": "python",
   "name": "daytrader"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}