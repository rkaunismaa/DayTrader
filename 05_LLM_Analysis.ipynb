{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Local LLM Financial Analysis on Your RTX 4090\n",
    "\n",
    "Your 4090 with 24GB VRAM can run powerful language models **locally** -- no API costs,\n",
    "no rate limits, no data leaving your machine. This notebook shows how to use them\n",
    "for financial document analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "1. **Load a Local LLM** - Run Phi-3 or Mistral entirely on your GPU\n",
    "2. **Earnings Call Analysis** - Summarize and extract key info from transcripts\n",
    "3. **SEC Filing Parser** - Pull actionable data from 10-K/10-Q filings\n",
    "4. **Financial Q&A** - Ask natural language questions about company data\n",
    "5. **News Digest** - Summarize batches of headlines into a morning brief\n",
    "6. **Risk Factor Extraction** - Identify key risks from filings\n",
    "7. **Model Comparison** - Benchmark different LLMs for financial tasks\n",
    "8. **Structured Output** - Get JSON-formatted analysis for pipeline integration\n",
    "9. **Combined Intelligence** - Merge LLM analysis with sentiment + technicals + Chronos\n",
    "\n",
    "---\n",
    "\n",
    "## Models That Fit on Your 4090\n",
    "\n",
    "| Model | Params | VRAM (fp16) | VRAM (4-bit) | Quality |\n",
    "|-------|--------|-------------|-------------|--------|\n",
    "| **microsoft/Phi-3-mini-4k-instruct** | 3.8B | ~7.5 GB | ~2.5 GB | Great for its size |\n",
    "| **microsoft/Phi-3.5-mini-instruct** | 3.8B | ~7.5 GB | ~2.5 GB | Improved Phi-3 |\n",
    "| **meta-llama/Llama-3.2-3B-Instruct** | 3B | ~6 GB | ~2 GB | Excellent reasoning |\n",
    "| **mistralai/Mistral-7B-Instruct-v0.3** | 7B | ~14 GB | ~4.5 GB | Strong all-around |\n",
    "| **meta-llama/Llama-3.1-8B-Instruct** | 8B | ~16 GB | ~5 GB | Very capable |\n",
    "| **mistralai/Mistral-Nemo-Instruct-2407** | 12B | N/A (too big fp16) | ~7.5 GB | Powerful 4-bit |\n",
    "\n",
    "We'll primarily use **Phi-3-mini** for speed and **Mistral-7B** (4-bit quantized) for quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"VRAM: {total_vram:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Phi-3 Mini -- fast, capable, fits easily on the 4090\n",
    "# First run downloads ~7.5GB. Cached at ~/.cache/huggingface/ after that.\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "print(f\"Loading {MODEL_ID}...\")\n",
    "print(\"(First run downloads the model. Subsequent runs use cache.)\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "load_time = time.time() - t0\n",
    "\n",
    "mem_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "print(f\"Model loaded in {load_time:.1f}s\")\n",
    "print(f\"GPU memory: {mem_used:.1f} GB / {total_vram:.1f} GB ({mem_used/total_vram*100:.0f}%)\")\n",
    "print(f\"Remaining VRAM: {total_vram - mem_used:.1f} GB (plenty for inference)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(prompt, system_prompt=None, max_new_tokens=512, temperature=0.3):\n",
    "    \"\"\"\n",
    "    Send a prompt to the local LLM and get a response.\n",
    "    \n",
    "    Parameters:\n",
    "        prompt: The user question/instruction\n",
    "        system_prompt: Optional system-level instruction\n",
    "        max_new_tokens: Maximum length of response\n",
    "        temperature: 0.0 = deterministic, 1.0 = creative\n",
    "    \n",
    "    Returns:\n",
    "        The model's text response\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # Apply chat template\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=temperature > 0,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    elapsed = time.time() - t0\n",
    "    \n",
    "    # Decode only the new tokens (skip the input)\n",
    "    new_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    tokens_generated = len(new_tokens)\n",
    "    tokens_per_sec = tokens_generated / elapsed if elapsed > 0 else 0\n",
    "    \n",
    "    return response, {'time': elapsed, 'tokens': tokens_generated, 'tok_per_sec': tokens_per_sec}\n",
    "\n",
    "\n",
    "# Quick test\n",
    "response, stats = ask_llm(\"What are the three most important financial ratios for evaluating a stock? Be concise.\")\n",
    "print(f\"Response ({stats['tokens']} tokens in {stats['time']:.1f}s, {stats['tok_per_sec']:.0f} tok/s):\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Earnings Call Analysis\n",
    "\n",
    "Earnings calls are goldmines of information. The CEO and CFO discuss:\n",
    "- Revenue, margins, and guidance\n",
    "- Strategic direction and new products\n",
    "- Risks and challenges\n",
    "- Management tone (confident vs cautious)\n",
    "\n",
    "A local LLM can process these transcripts in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated earnings call excerpt (in practice, use APIs like Financial Modeling Prep,\n",
    "# Seeking Alpha, or scrape from SEC EDGAR)\n",
    "\n",
    "EARNINGS_TRANSCRIPT = \"\"\"\n",
    "NVIDIA Corporation Q4 FY2024 Earnings Call Excerpt\n",
    "\n",
    "Jensen Huang, CEO:\n",
    "Thank you. Q4 was an extraordinary quarter. Revenue was $22.1 billion, up 265% year-over-year \n",
    "and up 22% sequentially. Data center revenue was $18.4 billion, up 409% year-over-year. \n",
    "The demand for accelerated computing and generative AI has driven a significant step-up in \n",
    "investment by cloud service providers, large enterprises, and sovereign AI infrastructure.\n",
    "\n",
    "Our Hopper architecture continues to see incredible adoption. H100 demand remains extremely \n",
    "strong, and we're ramping production of our next-generation Blackwell platform. We expect \n",
    "Blackwell to generate significant revenue in the second half of fiscal 2025.\n",
    "\n",
    "Gaming revenue was $2.9 billion, up 56% year-over-year, driven by GeForce RTX 40 series GPUs. \n",
    "Our RTX technology is becoming the standard for PC gaming and content creation.\n",
    "\n",
    "Colette Kress, CFO:\n",
    "Gross margin for the quarter was 76%, reflecting strong pricing in data center products. \n",
    "We expect gross margins to remain in the mid-70s percent range for the next several quarters. \n",
    "Operating expenses were $3.2 billion, up 26% year-over-year as we invest in research and \n",
    "development for our AI platform.\n",
    "\n",
    "For Q1 FY2025, we expect revenue of approximately $24 billion, plus or minus 2%. We continue \n",
    "to see strong demand across all our data center products. The pipeline for Blackwell is \n",
    "already several billion dollars.\n",
    "\n",
    "Supply remains tight relative to demand. We are working closely with our manufacturing partners \n",
    "to increase production capacity. We expect supply constraints to gradually ease through the \n",
    "second half of fiscal 2025.\n",
    "\n",
    "Q&A Highlights:\n",
    "- Analyst: What is the competitive landscape for AI accelerators?\n",
    "  Jensen: We have a significant moat through our CUDA software ecosystem. Over 4 million \n",
    "  developers use CUDA. Our competitors would need to replicate not just the hardware, but \n",
    "  the entire software stack that has been built over 15 years.\n",
    "\n",
    "- Analyst: What about China revenue impact from export controls?\n",
    "  Colette: China data center revenue was significant in prior years. The export controls \n",
    "  have reduced our addressable market in China. However, demand from other regions has more \n",
    "  than offset this impact. We are developing compliant products for the China market.\n",
    "\n",
    "- Analyst: How should we think about capital allocation?\n",
    "  Colette: We returned $2.8 billion to shareholders through buybacks and dividends this \n",
    "  quarter. We plan to continue our balanced approach to capital allocation, investing in \n",
    "  growth while returning capital to shareholders.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Transcript length: {len(EARNINGS_TRANSCRIPT)} characters\")\n",
    "print(f\"Approximately {len(EARNINGS_TRANSCRIPT.split())} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 1: Executive Summary\n",
    "SYSTEM_PROMPT = \"\"\"You are a financial analyst assistant. Analyze earnings call transcripts \n",
    "and provide clear, actionable insights for day traders. Be concise and focus on information \n",
    "that impacts stock price.\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Analyze this earnings call transcript and provide:\n",
    "1. A 2-3 sentence executive summary\n",
    "2. Key numbers (revenue, margins, guidance)\n",
    "3. Bull case (reasons the stock could go up)\n",
    "4. Bear case (reasons the stock could go down)\n",
    "5. Overall sentiment: BULLISH, BEARISH, or NEUTRAL\n",
    "\n",
    "Transcript:\n",
    "{EARNINGS_TRANSCRIPT}\"\"\"\n",
    "\n",
    "print(\"Analyzing earnings call...\\n\")\n",
    "response, stats = ask_llm(prompt, system_prompt=SYSTEM_PROMPT, max_new_tokens=600)\n",
    "print(f\"[{stats['time']:.1f}s, {stats['tok_per_sec']:.0f} tok/s]\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 2: Extract key metrics as structured data\n",
    "prompt_metrics = f\"\"\"Extract the following financial metrics from this earnings call transcript.\n",
    "Return ONLY a JSON object with these fields (use null if not mentioned):\n",
    "\n",
    "- quarterly_revenue\n",
    "- revenue_yoy_growth_pct\n",
    "- gross_margin_pct\n",
    "- data_center_revenue\n",
    "- gaming_revenue\n",
    "- next_quarter_guidance\n",
    "- operating_expenses\n",
    "- shareholder_returns\n",
    "- key_product_mentions (list of strings)\n",
    "- management_tone (one of: very_confident, confident, cautious, concerned)\n",
    "\n",
    "Transcript:\n",
    "{EARNINGS_TRANSCRIPT}\"\"\"\n",
    "\n",
    "print(\"Extracting structured metrics...\\n\")\n",
    "response_json, stats = ask_llm(prompt_metrics, max_new_tokens=400, temperature=0.1)\n",
    "print(f\"[{stats['time']:.1f}s]\\n\")\n",
    "print(response_json)\n",
    "\n",
    "# Try to parse the JSON\n",
    "try:\n",
    "    # Find JSON block in response\n",
    "    json_match = re.search(r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}', response_json, re.DOTALL)\n",
    "    if json_match:\n",
    "        metrics = json.loads(json_match.group())\n",
    "        print(\"\\n--- Parsed Metrics ---\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "except (json.JSONDecodeError, AttributeError) as e:\n",
    "    print(f\"\\nNote: Could not auto-parse JSON ({e}). This is common with smaller models.\")\n",
    "    print(\"Larger models (Mistral-7B, Llama-8B) are more reliable at structured output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 3: Management tone and sentiment cues\n",
    "prompt_tone = f\"\"\"Analyze the management tone in this earnings call. Focus on:\n",
    "\n",
    "1. Confidence level: Are executives confident or hedging?\n",
    "2. Forward guidance language: Strong commitments vs vague promises?\n",
    "3. Risk acknowledgment: Are they transparent about challenges?\n",
    "4. Key phrases that signal bullish or bearish intent\n",
    "5. Compare CEO tone vs CFO tone\n",
    "\n",
    "Quote specific phrases from the transcript to support your analysis.\n",
    "\n",
    "Transcript:\n",
    "{EARNINGS_TRANSCRIPT}\"\"\"\n",
    "\n",
    "print(\"Analyzing management tone...\\n\")\n",
    "response_tone, stats = ask_llm(prompt_tone, system_prompt=SYSTEM_PROMPT, max_new_tokens=500)\n",
    "print(f\"[{stats['time']:.1f}s]\\n\")\n",
    "print(response_tone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. SEC Filing Analysis\n",
    "\n",
    "SEC filings (10-K annual, 10-Q quarterly, 8-K events) contain critical information\n",
    "that moves stock prices. They're also extremely long and dense -- perfect for LLM analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated SEC 10-K Risk Factors excerpt\n",
    "# In production, use the SEC EDGAR API: https://efts.sec.gov/LATEST/search-index?q=...\n",
    "\n",
    "SEC_RISK_FACTORS = \"\"\"\n",
    "RISK FACTORS (Excerpt from NVIDIA 10-K Filing)\n",
    "\n",
    "Risks Related to Our Business and Industry\n",
    "\n",
    "Our operating results have in the past fluctuated and may in the future fluctuate, and if \n",
    "our operating results are below the expectations of securities analysts or investors, our \n",
    "stock price could decline.\n",
    "\n",
    "We derive a significant portion of our revenue from a limited number of customers and various \n",
    "different end markets. Revenue concentration in a small number of customers or different end \n",
    "markets may cause significant fluctuations in our results. Sales to our top customers \n",
    "represented approximately 45% of total revenue. Loss of a major customer or a significant \n",
    "reduction in purchases by any one of them could materially adversely affect our results.\n",
    "\n",
    "The semiconductor industry is highly competitive. We face competition from companies such as \n",
    "AMD, Intel, and various other chip designers and foundries. Some competitors have greater \n",
    "financial resources, more established customer relationships, and broader product portfolios. \n",
    "Additionally, major cloud service providers are developing their own AI accelerator chips, \n",
    "which could reduce their reliance on our products.\n",
    "\n",
    "Export controls and trade restrictions, particularly those related to China and other \n",
    "countries, have adversely affected and could in the future adversely affect our business. \n",
    "The U.S. government has implemented export controls on advanced AI chips to certain countries. \n",
    "China represented approximately 20-25% of our data center revenue in prior periods, and these \n",
    "restrictions have materially reduced our revenue from China.\n",
    "\n",
    "Our products are complex and may contain defects or may be subject to security vulnerabilities \n",
    "that could harm our reputation and adversely affect our business. Product defects or security \n",
    "vulnerabilities could result in significant warranty or other costs, damage our reputation, \n",
    "and lead to loss of customers.\n",
    "\n",
    "We depend on third-party foundries, primarily TSMC, for the manufacture of our products. \n",
    "Any disruption at TSMC, whether from natural disaster, geopolitical tension regarding Taiwan, \n",
    "or capacity constraints, could significantly impact our ability to meet demand and adversely \n",
    "affect our business.\n",
    "\n",
    "The AI market is rapidly evolving, and our success depends on our ability to anticipate \n",
    "customer needs and develop appropriate solutions. If the adoption of AI technologies is \n",
    "slower than expected, or if alternative approaches to AI computing emerge that do not favor \n",
    "our platform, our growth could be materially impacted.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"SEC filing excerpt: {len(SEC_RISK_FACTORS.split())} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze risk factors\n",
    "prompt_risks = f\"\"\"Analyze these SEC filing risk factors for NVIDIA. For each risk, rate its:\n",
    "- Severity (High/Medium/Low)\n",
    "- Likelihood in the next 12 months (High/Medium/Low)\n",
    "- Potential stock price impact\n",
    "\n",
    "Then provide:\n",
    "1. The TOP 3 risks a day trader should monitor\n",
    "2. What news events would trigger these risks\n",
    "3. How a trader should position if each risk materializes\n",
    "\n",
    "Risk Factors:\n",
    "{SEC_RISK_FACTORS}\"\"\"\n",
    "\n",
    "print(\"Analyzing SEC risk factors...\\n\")\n",
    "response_risks, stats = ask_llm(prompt_risks, system_prompt=SYSTEM_PROMPT, max_new_tokens=700)\n",
    "print(f\"[{stats['time']:.1f}s]\\n\")\n",
    "print(response_risks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract competitive landscape insights\n",
    "prompt_competitive = f\"\"\"Based on this SEC filing excerpt, analyze NVIDIA's competitive position:\n",
    "\n",
    "1. What are their competitive advantages (moats)?\n",
    "2. Who are the biggest competitive threats?\n",
    "3. What would cause their competitive position to weaken?\n",
    "4. Rate their competitive position: STRONG, MODERATE, or WEAK\n",
    "\n",
    "Keep it concise and actionable for a stock trader.\n",
    "\n",
    "Filing excerpt:\n",
    "{SEC_RISK_FACTORS}\"\"\"\n",
    "\n",
    "print(\"Analyzing competitive landscape...\\n\")\n",
    "response_comp, stats = ask_llm(prompt_competitive, system_prompt=SYSTEM_PROMPT, max_new_tokens=400)\n",
    "print(f\"[{stats['time']:.1f}s]\\n\")\n",
    "print(response_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Financial Q&A\n",
    "\n",
    "Ask natural language questions about any financial document.\n",
    "This is like having a research analyst on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialQA:\n",
    "    \"\"\"\n",
    "    Interactive Q&A system over financial documents.\n",
    "    Maintains context across questions for follow-ups.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, document, document_type=\"financial document\"):\n",
    "        self.document = document\n",
    "        self.document_type = document_type\n",
    "        self.system_prompt = f\"\"\"You are a financial analyst. You have been given a \n",
    "{document_type} to analyze. Answer questions based ONLY on information in the document. \n",
    "If the answer is not in the document, say so. Be concise and precise.\"\"\"\n",
    "    \n",
    "    def ask(self, question, max_new_tokens=300):\n",
    "        prompt = f\"\"\"Document:\n",
    "{self.document}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "        response, stats = ask_llm(\n",
    "            prompt, \n",
    "            system_prompt=self.system_prompt,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.1,\n",
    "        )\n",
    "        return response, stats\n",
    "\n",
    "\n",
    "# Create a Q&A system for the earnings call\n",
    "qa = FinancialQA(EARNINGS_TRANSCRIPT, \"earnings call transcript\")\n",
    "\n",
    "questions = [\n",
    "    \"What was the quarterly revenue and how does it compare to last year?\",\n",
    "    \"What is the revenue guidance for next quarter?\",\n",
    "    \"What did management say about competition from AMD?\",\n",
    "    \"How much did they return to shareholders?\",\n",
    "    \"What is the biggest risk mentioned in this call?\",\n",
    "]\n",
    "\n",
    "print(\"=== Financial Q&A Session ===\\n\")\n",
    "for q in questions:\n",
    "    print(f\"Q: {q}\")\n",
    "    answer, stats = qa.ask(q)\n",
    "    print(f\"A: {answer}\")\n",
    "    print(f\"   [{stats['time']:.1f}s]\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. News Digest -- Morning Brief Generator\n",
    "\n",
    "Feed the LLM a batch of headlines and get a synthesized morning brief.\n",
    "This is what a junior analyst does every morning -- now your GPU does it in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated morning news headlines\n",
    "MORNING_HEADLINES = [\n",
    "    \"NVIDIA Q4 revenue of $22.1B crushes estimates of $20.4B\",\n",
    "    \"NVIDIA guides Q1 revenue to $24B, above consensus of $22.2B\",\n",
    "    \"Fed minutes show officials divided on rate cut timing\",\n",
    "    \"China tech stocks rally on stimulus hopes\",\n",
    "    \"Tesla Cybertruck production hits 1,000 units per week\",\n",
    "    \"Apple reportedly in talks to bring Gemini AI to iPhone\",\n",
    "    \"S&P 500 futures up 0.8% on strong NVIDIA earnings\",\n",
    "    \"AMD launches MI300X AI chip, claims performance lead over H100\",\n",
    "    \"Bitcoin surges past $52,000 amid ETF inflows\",\n",
    "    \"US jobless claims fall to 194,000, below 200,000 estimate\",\n",
    "    \"Oil prices rise 2% on Middle East supply concerns\",\n",
    "    \"Meta announces new AI model that matches GPT-4 performance\",\n",
    "    \"10-year Treasury yield falls to 4.25% after weak housing data\",\n",
    "    \"Microsoft Copilot adoption grows to 40% of Fortune 500\",\n",
    "    \"Retail sales rise 0.6%, beating expectations of 0.3%\",\n",
    "]\n",
    "\n",
    "# Generate a morning brief\n",
    "headlines_text = \"\\n\".join(f\"- {h}\" for h in MORNING_HEADLINES)\n",
    "\n",
    "prompt_brief = f\"\"\"You are a senior market strategist preparing a morning brief for day traders.\n",
    "Based on these headlines, write a concise morning brief that covers:\n",
    "\n",
    "1. MARKET OUTLOOK: Overall tone for today's session (bullish/bearish/mixed)\n",
    "2. KEY CATALYST: The #1 story moving markets today\n",
    "3. SECTOR FOCUS: Which sectors to watch and why\n",
    "4. RISK EVENTS: What could go wrong today\n",
    "5. TRADE IDEAS: 2-3 specific stocks to watch with directional bias\n",
    "\n",
    "Headlines:\n",
    "{headlines_text}\"\"\"\n",
    "\n",
    "print(\"Generating morning brief...\\n\")\n",
    "brief, stats = ask_llm(prompt_brief, max_new_tokens=600, temperature=0.3)\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  MORNING BRIEF -- Generated in {stats['time']:.1f}s\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "print(brief)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify headlines by sector and impact\n",
    "prompt_classify = f\"\"\"Classify each headline by:\n",
    "- Sector: Tech, Finance, Energy, Macro, Crypto, Consumer\n",
    "- Impact: HIGH, MEDIUM, LOW\n",
    "- Direction: BULLISH, BEARISH, NEUTRAL\n",
    "- Relevant tickers (if any)\n",
    "\n",
    "Return one line per headline in this exact format:\n",
    "HEADLINE | SECTOR | IMPACT | DIRECTION | TICKERS\n",
    "\n",
    "Headlines:\n",
    "{headlines_text}\"\"\"\n",
    "\n",
    "print(\"Classifying headlines...\\n\")\n",
    "classified, stats = ask_llm(prompt_classify, max_new_tokens=600, temperature=0.1)\n",
    "print(f\"[{stats['time']:.1f}s]\\n\")\n",
    "print(classified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Loading Larger Models with 4-bit Quantization\n",
    "\n",
    "For higher quality analysis, we can load Mistral-7B using 4-bit quantization.\n",
    "This compresses the model from ~14GB to ~4.5GB, fitting easily on your 4090\n",
    "with minimal quality loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU memory from Phi-3\n",
    "print(f\"Before cleanup: {torch.cuda.memory_allocated(0)/1024**3:.1f} GB used\")\n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"After cleanup: {torch.cuda.memory_allocated(0)/1024**3:.1f} GB used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Mistral-7B with 4-bit quantization\n",
    "# This gives us a much more capable model while using ~4.5GB VRAM\n",
    "\n",
    "LARGE_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",          # Normal Float 4 -- best quality\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Compute in fp16 for speed\n",
    "    bnb_4bit_use_double_quant=True,       # Further compression\n",
    ")\n",
    "\n",
    "print(f\"Loading {LARGE_MODEL_ID} (4-bit quantized)...\")\n",
    "print(\"(First run downloads ~14GB. Cached after that.)\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(LARGE_MODEL_ID)\n",
    "model_large = AutoModelForCausalLM.from_pretrained(\n",
    "    LARGE_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "load_time = time.time() - t0\n",
    "\n",
    "mem_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "print(f\"Model loaded in {load_time:.1f}s\")\n",
    "print(f\"GPU memory: {mem_used:.1f} GB / {total_vram:.1f} GB ({mem_used/total_vram*100:.0f}%)\")\n",
    "print(f\"\\n4-bit quantization saves ~10GB of VRAM with minimal quality loss.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm_large(prompt, system_prompt=None, max_new_tokens=512, temperature=0.3):\n",
    "    \"\"\"Send a prompt to the larger Mistral model.\"\"\"\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    input_text = tokenizer_large.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer_large(input_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model_large.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=temperature > 0,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer_large.eos_token_id,\n",
    "        )\n",
    "    elapsed = time.time() - t0\n",
    "    \n",
    "    new_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    response = tokenizer_large.decode(new_tokens, skip_special_tokens=True)\n",
    "    tokens_generated = len(new_tokens)\n",
    "    \n",
    "    return response, {'time': elapsed, 'tokens': tokens_generated,\n",
    "                       'tok_per_sec': tokens_generated / elapsed if elapsed > 0 else 0}\n",
    "\n",
    "\n",
    "# Test the larger model\n",
    "print(\"Testing Mistral-7B...\\n\")\n",
    "response, stats = ask_llm_large(\n",
    "    \"Compare NVIDIA and AMD as investment opportunities for 2024. \"\n",
    "    \"Consider revenue growth, margins, AI exposure, and valuation. Be concise.\",\n",
    "    max_new_tokens=400\n",
    ")\n",
    "print(f\"[{stats['time']:.1f}s, {stats['tok_per_sec']:.0f} tok/s]\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Structured Output for Pipeline Integration\n",
    "\n",
    "For automated trading pipelines, we need the LLM to return structured data\n",
    "(JSON) that can be consumed by code. Larger models are better at this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_earnings_structured(transcript, llm_fn=ask_llm_large):\n",
    "    \"\"\"\n",
    "    Analyze an earnings call and return structured JSON output.\n",
    "    Designed for pipeline integration.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Analyze this earnings call transcript and return a JSON object with EXACTLY these fields:\n",
    "\n",
    "{{\n",
    "  \"ticker\": \"<stock ticker>\",\n",
    "  \"quarter\": \"<e.g. Q4 FY2024>\",\n",
    "  \"revenue_billions\": <number>,\n",
    "  \"revenue_yoy_pct\": <number>,\n",
    "  \"gross_margin_pct\": <number>,\n",
    "  \"guidance_revenue_billions\": <number or null>,\n",
    "  \"guidance_vs_consensus\": \"<beat/miss/inline>\",\n",
    "  \"sentiment_score\": <-1.0 to 1.0>,\n",
    "  \"management_confidence\": <1 to 10>,\n",
    "  \"top_3_positives\": [\"<string>\", \"<string>\", \"<string>\"],\n",
    "  \"top_3_risks\": [\"<string>\", \"<string>\", \"<string>\"],\n",
    "  \"trading_signal\": \"<STRONG_BUY/BUY/HOLD/SELL/STRONG_SELL>\",\n",
    "  \"signal_reasoning\": \"<1 sentence>\"\n",
    "}}\n",
    "\n",
    "Return ONLY the JSON object, no other text.\n",
    "\n",
    "Transcript:\n",
    "{transcript}\"\"\"\n",
    "\n",
    "    response, stats = llm_fn(prompt, max_new_tokens=500, temperature=0.1)\n",
    "    \n",
    "    # Parse JSON from response\n",
    "    try:\n",
    "        json_match = re.search(r'\\{[\\s\\S]*\\}', response)\n",
    "        if json_match:\n",
    "            data = json.loads(json_match.group())\n",
    "            return data, stats\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    return {'raw_response': response, 'parse_error': True}, stats\n",
    "\n",
    "\n",
    "# Run structured analysis\n",
    "print(\"Running structured earnings analysis...\\n\")\n",
    "analysis, stats = analyze_earnings_structured(EARNINGS_TRANSCRIPT)\n",
    "print(f\"[{stats['time']:.1f}s]\\n\")\n",
    "\n",
    "if 'parse_error' not in analysis:\n",
    "    print(json.dumps(analysis, indent=2))\n",
    "    \n",
    "    print(\"\\n--- Pipeline-Ready Output ---\")\n",
    "    print(f\"Ticker:     {analysis.get('ticker', 'N/A')}\")\n",
    "    print(f\"Signal:     {analysis.get('trading_signal', 'N/A')}\")\n",
    "    print(f\"Sentiment:  {analysis.get('sentiment_score', 'N/A')}\")\n",
    "    print(f\"Confidence: {analysis.get('management_confidence', 'N/A')}/10\")\n",
    "    print(f\"\\nThis structured output can feed directly into your trading pipeline.\")\n",
    "else:\n",
    "    print(\"Could not parse structured JSON. Raw response:\")\n",
    "    print(analysis['raw_response'])\n",
    "    print(\"\\nTip: Larger models or lower temperature improve JSON reliability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Comparative Analysis -- Multiple Stocks\n",
    "\n",
    "Use the LLM to compare multiple companies and generate relative trading ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile basic financials for comparison\n",
    "import yfinance as yf\n",
    "\n",
    "COMPARE_TICKERS = ['NVDA', 'AMD', 'INTC']\n",
    "\n",
    "company_summaries = []\n",
    "for ticker in COMPARE_TICKERS:\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        info = stock.info\n",
    "        \n",
    "        summary = (\n",
    "            f\"{ticker} ({info.get('shortName', 'N/A')}):\\n\"\n",
    "            f\"  Market Cap: ${info.get('marketCap', 0)/1e9:.0f}B\\n\"\n",
    "            f\"  Revenue (TTM): ${info.get('totalRevenue', 0)/1e9:.1f}B\\n\"\n",
    "            f\"  Gross Margin: {info.get('grossMargins', 0)*100:.1f}%\\n\"\n",
    "            f\"  P/E Ratio: {info.get('trailingPE', 'N/A')}\\n\"\n",
    "            f\"  Forward P/E: {info.get('forwardPE', 'N/A')}\\n\"\n",
    "            f\"  Revenue Growth: {info.get('revenueGrowth', 0)*100:.1f}%\\n\"\n",
    "            f\"  52-Week Range: ${info.get('fiftyTwoWeekLow', 0):.2f} - ${info.get('fiftyTwoWeekHigh', 0):.2f}\\n\"\n",
    "            f\"  Analyst Target: ${info.get('targetMeanPrice', 'N/A')}\\n\"\n",
    "        )\n",
    "        company_summaries.append(summary)\n",
    "        print(summary)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not fetch {ticker}: {e}\")\n",
    "        company_summaries.append(f\"{ticker}: Data unavailable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM comparative analysis\n",
    "all_summaries = \"\\n\".join(company_summaries)\n",
    "\n",
    "prompt_compare = f\"\"\"Compare these three semiconductor companies for a trader considering \n",
    "positions in this sector. Analyze:\n",
    "\n",
    "1. RELATIVE VALUE: Which is cheapest relative to growth?\n",
    "2. MOMENTUM: Which has the strongest business trajectory?\n",
    "3. RISK/REWARD: Which offers the best asymmetric setup?\n",
    "4. PAIR TRADE: If you had to go long one and short another, which pair and why?\n",
    "5. RANKING: Rank all three from most to least attractive right now.\n",
    "\n",
    "Company Data:\n",
    "{all_summaries}\"\"\"\n",
    "\n",
    "print(\"Generating comparative analysis...\\n\")\n",
    "comparison, stats = ask_llm_large(prompt_compare, max_new_tokens=600)\n",
    "print(f\"[{stats['time']:.1f}s]\\n\")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Combined Intelligence Dashboard\n",
    "\n",
    "Merge LLM analysis with results from our other notebooks:\n",
    "- **Notebook 02**: Sentiment scores\n",
    "- **Notebook 03**: Technical signals\n",
    "- **Notebook 04**: Chronos forecasts\n",
    "- **This notebook**: LLM fundamental analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combined_report(ticker, headlines, llm_fn=ask_llm_large):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive stock report combining all signal sources.\n",
    "    \n",
    "    In a full pipeline, you'd pass in real data from notebooks 02-04.\n",
    "    Here we use the LLM to synthesize a report from available information.\n",
    "    \"\"\"\n",
    "    # Fetch basic data\n",
    "    stock = yf.Ticker(ticker)\n",
    "    info = stock.info\n",
    "    hist = stock.history(period='1mo')\n",
    "    \n",
    "    price = info.get('currentPrice', info.get('previousClose', 0))\n",
    "    change_1m = (hist['Close'].iloc[-1] / hist['Close'].iloc[0] - 1) * 100 if len(hist) > 1 else 0\n",
    "    \n",
    "    headlines_text = \"\\n\".join(f\"- {h}\" for h in headlines[:10])\n",
    "    \n",
    "    prompt = f\"\"\"Generate a concise trading report for {ticker}.\n",
    "\n",
    "Current Data:\n",
    "- Price: ${price:.2f}\n",
    "- 1-Month Change: {change_1m:+.1f}%\n",
    "- Market Cap: ${info.get('marketCap', 0)/1e9:.0f}B\n",
    "- P/E: {info.get('trailingPE', 'N/A')}\n",
    "- Revenue Growth: {info.get('revenueGrowth', 0)*100:.1f}%\n",
    "\n",
    "Recent Headlines:\n",
    "{headlines_text}\n",
    "\n",
    "Provide:\n",
    "1. THESIS (1-2 sentences: bullish or bearish case)\n",
    "2. KEY LEVELS: Support and resistance to watch\n",
    "3. CATALYSTS: Upcoming events that could move the stock\n",
    "4. TRADE SETUP: Entry, stop loss, and target if taking a position\n",
    "5. OVERALL RATING: STRONG BUY / BUY / HOLD / SELL / STRONG SELL\n",
    "\n",
    "Be specific with numbers.\"\"\"\n",
    "    \n",
    "    response, stats = llm_fn(prompt, max_new_tokens=500, temperature=0.3)\n",
    "    return response, stats\n",
    "\n",
    "\n",
    "# Generate reports for key stocks\n",
    "sample_headlines = {\n",
    "    'NVDA': [\n",
    "        \"NVIDIA Q4 revenue of $22.1B crushes estimates\",\n",
    "        \"NVIDIA guides Q1 to $24B, above consensus\",\n",
    "        \"AMD launches MI300X, claims performance gains over H100\",\n",
    "        \"Analysts raise NVDA price targets post earnings\",\n",
    "        \"NVIDIA Blackwell GPU production ramping ahead of schedule\",\n",
    "    ],\n",
    "    'TSLA': [\n",
    "        \"Tesla Cybertruck production hits 1,000 per week\",\n",
    "        \"Tesla cuts prices in China amid competition\",\n",
    "        \"Musk announces robotaxi unveil event\",\n",
    "        \"Tesla Q4 deliveries miss estimates slightly\",\n",
    "        \"Tesla FSD v12 shows significant improvement in testing\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "for ticker, headlines in sample_headlines.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  {ticker} TRADING REPORT\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    report, stats = generate_combined_report(ticker, headlines)\n",
    "    print(f\"[Generated in {stats['time']:.1f}s]\\n\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Batch Document Processing\n",
    "\n",
    "Process multiple documents efficiently -- useful for scanning\n",
    "earnings across an entire sector in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch headline analysis -- process many headlines at once\n",
    "\n",
    "SECTOR_HEADLINES = {\n",
    "    'Semiconductors': [\n",
    "        \"NVIDIA data center revenue up 409% year-over-year\",\n",
    "        \"AMD MI300X wins major hyperscaler design win\",\n",
    "        \"Intel foundry business reports $7B annual loss\",\n",
    "        \"TSMC raises capex guidance to $32B on AI demand\",\n",
    "        \"Broadcom AI revenue doubles to $3.7B\",\n",
    "    ],\n",
    "    'Cloud/Software': [\n",
    "        \"Microsoft Azure revenue growth accelerates to 30%\",\n",
    "        \"Salesforce cuts workforce by 10% to improve margins\",\n",
    "        \"Snowflake product revenue grows 32% year-over-year\",\n",
    "        \"ServiceNow raises full-year guidance above consensus\",\n",
    "        \"Palantir government revenue growth slows to 11%\",\n",
    "    ],\n",
    "    'Consumer Tech': [\n",
    "        \"Apple iPhone sales decline 3% in China\",\n",
    "        \"Meta ad revenue beats by 5%, Reels monetization improving\",\n",
    "        \"Amazon Prime membership hits 200 million worldwide\",\n",
    "        \"Google Search revenue misses estimates by 2%\",\n",
    "        \"Netflix adds 13 million subscribers, above guidance\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"Generating sector-by-sector analysis...\\n\")\n",
    "\n",
    "for sector, headlines in SECTOR_HEADLINES.items():\n",
    "    headlines_text = \"\\n\".join(f\"  - {h}\" for h in headlines)\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these {sector} headlines as a sector analyst.\n",
    "In 3-4 sentences: What's the overall sector trend? Which company stands out \n",
    "(positively or negatively)? What's the trade?\n",
    "\n",
    "Headlines:\n",
    "{headlines_text}\"\"\"\n",
    "    \n",
    "    response, stats = ask_llm_large(prompt, max_new_tokens=200, temperature=0.3)\n",
    "    \n",
    "    print(f\"--- {sector} [{stats['time']:.1f}s] ---\")\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Performance Benchmarks\n",
    "\n",
    "How fast is your 4090 at various LLM tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: tokens per second at different output lengths\n",
    "prompt = \"Explain the three most important things a day trader should know about reading earnings reports.\"\n",
    "\n",
    "output_lengths = [50, 100, 200, 400]\n",
    "benchmark_results = []\n",
    "\n",
    "print(\"Benchmarking Mistral-7B (4-bit) generation speed...\\n\")\n",
    "print(f\"{'Max Tokens':>12} {'Time (s)':>10} {'Tokens/sec':>12} {'Output Len':>12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for max_tok in output_lengths:\n",
    "    response, stats = ask_llm_large(prompt, max_new_tokens=max_tok, temperature=0.3)\n",
    "    benchmark_results.append({\n",
    "        'max_tokens': max_tok,\n",
    "        'time': stats['time'],\n",
    "        'tok_per_sec': stats['tok_per_sec'],\n",
    "        'actual_tokens': stats['tokens'],\n",
    "    })\n",
    "    print(f\"{max_tok:>12} {stats['time']:>10.2f} {stats['tok_per_sec']:>12.1f} {stats['tokens']:>12}\")\n",
    "\n",
    "# Summary\n",
    "avg_speed = np.mean([r['tok_per_sec'] for r in benchmark_results])\n",
    "print(f\"\\nAverage: {avg_speed:.0f} tokens/sec\")\n",
    "print(f\"\\nAt this speed, the model can:\")\n",
    "print(f\"  - Analyze an earnings transcript in ~5-10 seconds\")\n",
    "print(f\"  - Classify 50 headlines in ~30 seconds\")\n",
    "print(f\"  - Generate a full trading report in ~8-15 seconds\")\n",
    "print(f\"  - Process a day's worth of news in under 5 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Tips for Getting Better LLM Output\n",
    "\n",
    "### Prompt Engineering for Financial Analysis\n",
    "\n",
    "| Technique | Example | Why It Works |\n",
    "|-----------|---------|-------------|\n",
    "| **Role assignment** | \"You are a senior equity analyst at Goldman Sachs\" | Primes the model for domain-specific language |\n",
    "| **Structured output** | \"Return a JSON object with these fields: ...\" | Gets parseable data for pipelines |\n",
    "| **Few-shot examples** | Show 1-2 example inputs and outputs | Teaches the exact format you want |\n",
    "| **Chain of thought** | \"Think step by step about the implications\" | Improves reasoning quality |\n",
    "| **Constraints** | \"In exactly 3 bullet points\" | Prevents rambling |\n",
    "| **Low temperature** | `temperature=0.1` for factual extraction | Reduces hallucination |\n",
    "| **High temperature** | `temperature=0.7` for creative analysis | Generates novel insights |\n",
    "\n",
    "### Model Selection Guide\n",
    "\n",
    "| Task | Best Model | Why |\n",
    "|------|-----------|-----|\n",
    "| Quick headline classification | Phi-3 Mini (fp16) | Fast, simple task |\n",
    "| Earnings call deep dive | Mistral-7B (4-bit) | Needs reasoning depth |\n",
    "| JSON structured output | Mistral-7B or Llama-8B | Better instruction following |\n",
    "| Creative trade thesis | Mistral-7B (higher temp) | Needs creative reasoning |\n",
    "| Bulk processing (1000+ items) | Phi-3 Mini (fp16) | Speed matters more |\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "- **Hallucination**: LLMs can invent financial numbers. Always verify against source data.\n",
    "- **Recency cutoff**: The model's training data has a cutoff date. It doesn't know about recent events.\n",
    "- **Overconfidence**: LLMs present speculation as fact. Treat outputs as analysis, not truth.\n",
    "- **Context window**: Phi-3 has a 4K token window. Larger documents need chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Summary & Architecture\n",
    "\n",
    "### What We Built\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| **`ask_llm()` / `ask_llm_large()`** | Core inference functions for any prompt |\n",
    "| **Earnings Analysis** | Summary, tone analysis, structured metric extraction |\n",
    "| **SEC Filing Parser** | Risk factor analysis, competitive landscape assessment |\n",
    "| **Financial Q&A** | `FinancialQA` class for interactive document queries |\n",
    "| **Morning Brief Generator** | Synthesize headlines into actionable trading brief |\n",
    "| **Structured Output** | JSON-formatted analysis for pipeline integration |\n",
    "| **Comparative Analysis** | Multi-stock comparison with relative value insights |\n",
    "| **Combined Report** | Full trading report integrating multiple data sources |\n",
    "| **Batch Processing** | Sector-level analysis across multiple headline sets |\n",
    "\n",
    "### Full Pipeline Architecture\n",
    "\n",
    "```\n",
    "Data Sources                  GPU Processing Pipeline              Output\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 yfinance     \u2502    \u2502 Notebook 02: Sentiment           \u2502    \u2502 Per-stock    \u2502\n",
    "\u2502 RSS feeds    \u2502\u2500\u2500\u2500>\u2502  FinBERT + RoBERTa + DistilRoBERTa\u2502\u2500\u2500>\u2502 sentiment    \u2502\n",
    "\u2502 News APIs    \u2502    \u2502                                   \u2502    \u2502 scores       \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Price/volume \u2502    \u2502 Notebook 03: Technical            \u2502    \u2502 Buy/sell     \u2502\n",
    "\u2502 OHLCV bars   \u2502\u2500\u2500\u2500>\u2502  EMA, RSI, BB, VWAP, ORB         \u2502\u2500\u2500>\u2502 signals      \u2502\n",
    "\u2502              \u2502    \u2502  BacktestEngine validation         \u2502    \u2502              \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Price history\u2502    \u2502 Notebook 04: Chronos              \u2502    \u2502 Probabilistic\u2502\n",
    "\u2502              \u2502\u2500\u2500\u2500>\u2502  Zero-shot forecasting             \u2502\u2500\u2500>\u2502 forecasts    \u2502\n",
    "\u2502              \u2502    \u2502  Confidence intervals              \u2502    \u2502              \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Earnings     \u2502    \u2502 Notebook 05: LLM Analysis         \u2502    \u2502 Fundamental  \u2502\n",
    "\u2502 SEC filings  \u2502\u2500\u2500\u2500>\u2502  Phi-3 / Mistral-7B               \u2502\u2500\u2500>\u2502 insights &   \u2502\n",
    "\u2502 Transcripts  \u2502    \u2502  Structured JSON output            \u2502    \u2502 trade thesis \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                                                    \u2502\n",
    "                                                                    v\n",
    "                                                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                                                           \u2502 COMBINED     \u2502\n",
    "                                                           \u2502 SIGNAL       \u2502\n",
    "                                                           \u2502 + Risk Mgmt  \u2502\n",
    "                                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "### Coming Up Next\n",
    "\n",
    "Potential future notebooks:\n",
    "- **06_Backtesting_Engine.ipynb** - Full systematic backtesting with combined signals\n",
    "- **07_Paper_Trading_Bot.ipynb** - Automated paper trading with Alpaca API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    mem_before = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    try:\n",
    "        del model_large, tokenizer_large\n",
    "    except NameError:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    mem_after = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"GPU memory freed: {mem_before:.1f} GB -> {mem_after:.1f} GB\")\n",
    "\n",
    "print(\"\\nNotebook 05 complete.\")\n",
    "print(\"You now have a full local LLM financial analysis pipeline on your 4090.\")\n",
    "print(\"No API costs, no rate limits, no data leaving your machine.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DayTrader (Python 3.10)",
   "language": "python",
   "name": "daytrader"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}