{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Real-Time News Sentiment Pipeline (GPU-Accelerated)\n",
    "\n",
    "This notebook builds a **production-style sentiment analysis pipeline** for day trading,\n",
    "powered by your RTX 4090.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "1. **News Fetching** - Pull financial news from RSS feeds and APIs\n",
    "2. **Multi-Model Sentiment** - Compare FinBERT, Twitter-RoBERTa, and DistilRoBERTa\n",
    "3. **Entity Extraction** - Automatically detect which stocks a headline is about\n",
    "4. **Per-Stock Sentiment Scoring** - Aggregate sentiment by ticker over time\n",
    "5. **Sentiment-Price Correlation** - Does sentiment actually predict price moves?\n",
    "6. **Batch Processing Pipeline** - Process thousands of headlines per second on GPU\n",
    "7. **Live Monitoring Architecture** - Design for a real-time sentiment dashboard\n",
    "\n",
    "---\n",
    "\n",
    "## Why Sentiment Matters for Day Trading\n",
    "\n",
    "- **News moves markets**: Earnings beats, FDA approvals, analyst upgrades cause instant price moves\n",
    "- **Speed is edge**: If you can analyze sentiment in milliseconds, you react before the crowd\n",
    "- **Volume of information**: Humans can't read 10,000 headlines/day; your GPU can analyze them in seconds\n",
    "- **Contrarian signals**: Extreme sentiment often marks tops/bottoms (\"buy the fear, sell the greed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('dark_background')\n",
    "sns.set_palette('bright')\n",
    "\n",
    "# GPU check\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_mem / 1024**3\n",
    "    print(f\"VRAM: {vram_gb:.1f} GB\")\n",
    "    print(f\"\\nMore than enough for multiple sentiment models simultaneously.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Fetching Financial News\n",
    "\n",
    "We'll use multiple free sources to gather financial headlines:\n",
    "\n",
    "| Source | Method | Cost | Latency |\n",
    "|--------|--------|------|---------|\n",
    "| **RSS Feeds** | feedparser | Free | Minutes |\n",
    "| **Yahoo Finance** | yfinance news | Free | Minutes |\n",
    "| **NewsAPI.org** | REST API | Free tier (100/day) | Seconds |\n",
    "| **Reddit** | PRAW API | Free | Seconds |\n",
    "| **SEC EDGAR** | REST API | Free | Minutes |\n",
    "\n",
    "For this notebook, we'll use RSS feeds (no API key needed) and yfinance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import yfinance as yf\n",
    "\n",
    "# --- RSS Feed Sources ---\n",
    "# These are free, public financial news RSS feeds\n",
    "RSS_FEEDS = {\n",
    "    'Yahoo Finance': 'https://finance.yahoo.com/news/rssindex',\n",
    "    'MarketWatch': 'https://feeds.marketwatch.com/marketwatch/topstories/',\n",
    "    'CNBC': 'https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=100003114',\n",
    "    'Reuters Business': 'https://www.reutersagency.com/feed/?best-topics=business-finance',\n",
    "    'Seeking Alpha': 'https://seekingalpha.com/market_currents.xml',\n",
    "}\n",
    "\n",
    "def fetch_rss_headlines(feed_url, source_name, max_items=50):\n",
    "    \"\"\"Fetch headlines from an RSS feed.\"\"\"\n",
    "    try:\n",
    "        feed = feedparser.parse(feed_url)\n",
    "        headlines = []\n",
    "        for entry in feed.entries[:max_items]:\n",
    "            headlines.append({\n",
    "                'title': entry.get('title', ''),\n",
    "                'summary': entry.get('summary', ''),\n",
    "                'published': entry.get('published', ''),\n",
    "                'link': entry.get('link', ''),\n",
    "                'source': source_name,\n",
    "            })\n",
    "        return headlines\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not fetch {source_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Fetch from all RSS sources\n",
    "print(\"Fetching news from RSS feeds...\\n\")\n",
    "all_headlines = []\n",
    "for name, url in RSS_FEEDS.items():\n",
    "    headlines = fetch_rss_headlines(url, name)\n",
    "    all_headlines.extend(headlines)\n",
    "    print(f\"  {name}: {len(headlines)} headlines\")\n",
    "\n",
    "print(f\"\\nTotal headlines fetched: {len(all_headlines)}\")\n",
    "\n",
    "# Show a sample\n",
    "if all_headlines:\n",
    "    print(\"\\n--- Sample Headlines ---\")\n",
    "    for h in all_headlines[:5]:\n",
    "        print(f\"  [{h['source']}] {h['title'][:80]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Yahoo Finance Stock-Specific News ---\n",
    "# Get news for stocks we're interested in day trading\n",
    "\n",
    "WATCHLIST = ['NVDA', 'AAPL', 'TSLA', 'AMD', 'META', 'MSFT', 'AMZN', 'GOOGL', 'SPY', 'QQQ']\n",
    "\n",
    "def fetch_yfinance_news(tickers):\n",
    "    \"\"\"Fetch recent news for a list of tickers from Yahoo Finance.\"\"\"\n",
    "    news_items = []\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            news = stock.news\n",
    "            if news:\n",
    "                for item in news[:10]:\n",
    "                    news_items.append({\n",
    "                        'title': item.get('title', ''),\n",
    "                        'summary': item.get('summary', ''),\n",
    "                        'published': item.get('providerPublishTime', ''),\n",
    "                        'link': item.get('link', ''),\n",
    "                        'source': f\"Yahoo/{ticker}\",\n",
    "                        'ticker': ticker,\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not fetch news for {ticker}: {e}\")\n",
    "    return news_items\n",
    "\n",
    "print(\"Fetching stock-specific news from Yahoo Finance...\\n\")\n",
    "stock_news = fetch_yfinance_news(WATCHLIST)\n",
    "print(f\"Stock-specific headlines: {len(stock_news)}\")\n",
    "\n",
    "if stock_news:\n",
    "    print(\"\\n--- Sample Stock News ---\")\n",
    "    for h in stock_news[:5]:\n",
    "        print(f\"  [{h.get('ticker', '?')}] {h['title'][:80]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all news into a DataFrame\n",
    "combined_news = all_headlines + stock_news\n",
    "\n",
    "# If we got very few live headlines, supplement with realistic simulated data\n",
    "# so the rest of the notebook works well as a demonstration\n",
    "SIMULATED_HEADLINES = [\n",
    "    {'title': 'NVIDIA beats Q4 earnings estimates, data center revenue surges 400%', 'source': 'Reuters', 'ticker': 'NVDA'},\n",
    "    {'title': 'NVIDIA announces next-gen Blackwell GPU architecture', 'source': 'CNBC', 'ticker': 'NVDA'},\n",
    "    {'title': 'Analysts raise NVDA price targets following blowout earnings', 'source': 'MarketWatch', 'ticker': 'NVDA'},\n",
    "    {'title': 'Concerns grow over NVIDIA valuation as PE ratio hits 60x', 'source': 'Seeking Alpha', 'ticker': 'NVDA'},\n",
    "    {'title': 'NVIDIA faces increased competition from AMD MI300X in data centers', 'source': 'Reuters', 'ticker': 'NVDA'},\n",
    "    {'title': 'Tesla deliveries fall short of expectations in Q4', 'source': 'CNBC', 'ticker': 'TSLA'},\n",
    "    {'title': 'Elon Musk announces Tesla robotaxi launch date', 'source': 'Reuters', 'ticker': 'TSLA'},\n",
    "    {'title': 'Tesla cuts prices across all models amid demand concerns', 'source': 'MarketWatch', 'ticker': 'TSLA'},\n",
    "    {'title': 'Tesla Cybertruck production ramps up, beating internal targets', 'source': 'Yahoo', 'ticker': 'TSLA'},\n",
    "    {'title': 'TSLA short interest rises to highest level this year', 'source': 'Seeking Alpha', 'ticker': 'TSLA'},\n",
    "    {'title': 'Apple Vision Pro sales exceed analyst expectations', 'source': 'CNBC', 'ticker': 'AAPL'},\n",
    "    {'title': 'Apple services revenue hits record high in Q1', 'source': 'Reuters', 'ticker': 'AAPL'},\n",
    "    {'title': 'Apple faces antitrust lawsuit from Department of Justice', 'source': 'MarketWatch', 'ticker': 'AAPL'},\n",
    "    {'title': 'iPhone sales decline in China as Huawei gains market share', 'source': 'Reuters', 'ticker': 'AAPL'},\n",
    "    {'title': 'Apple reportedly developing in-house AI chip for data centers', 'source': 'Bloomberg', 'ticker': 'AAPL'},\n",
    "    {'title': 'AMD MI300X wins major cloud customer away from NVIDIA', 'source': 'Reuters', 'ticker': 'AMD'},\n",
    "    {'title': 'AMD reports strong quarterly results, raises full-year guidance', 'source': 'CNBC', 'ticker': 'AMD'},\n",
    "    {'title': 'AMD stock drops on weaker than expected gaming segment', 'source': 'MarketWatch', 'ticker': 'AMD'},\n",
    "    {'title': 'Meta platforms invests $10B in AI infrastructure buildout', 'source': 'Reuters', 'ticker': 'META'},\n",
    "    {'title': 'Meta ad revenue surges as Reels monetization improves', 'source': 'CNBC', 'ticker': 'META'},\n",
    "    {'title': 'Federal Reserve holds rates steady, signals patience on cuts', 'source': 'Reuters', 'ticker': 'SPY'},\n",
    "    {'title': 'S&P 500 hits new all-time high on strong jobs data', 'source': 'CNBC', 'ticker': 'SPY'},\n",
    "    {'title': 'Inflation data comes in hotter than expected, markets sell off', 'source': 'MarketWatch', 'ticker': 'SPY'},\n",
    "    {'title': 'Tech stocks lead market rally as Nasdaq surges 2%', 'source': 'Reuters', 'ticker': 'QQQ'},\n",
    "    {'title': 'Semiconductor stocks plunge on new China export restrictions', 'source': 'CNBC', 'ticker': 'QQQ'},\n",
    "    {'title': 'Microsoft Azure revenue grows 30%, beating cloud estimates', 'source': 'Reuters', 'ticker': 'MSFT'},\n",
    "    {'title': 'Microsoft Copilot AI adoption slower than expected among enterprises', 'source': 'MarketWatch', 'ticker': 'MSFT'},\n",
    "    {'title': 'Amazon AWS revenue accelerates, stock jumps in after-hours', 'source': 'CNBC', 'ticker': 'AMZN'},\n",
    "    {'title': 'Amazon faces FTC investigation over marketplace practices', 'source': 'Reuters', 'ticker': 'AMZN'},\n",
    "    {'title': 'Google Search revenue misses estimates as AI competition heats up', 'source': 'MarketWatch', 'ticker': 'GOOGL'},\n",
    "    {'title': 'Alphabet announces $70B share buyback program', 'source': 'Reuters', 'ticker': 'GOOGL'},\n",
    "    {'title': 'Market breadth narrows as only mega-cap tech stocks advance', 'source': 'Seeking Alpha', 'ticker': 'SPY'},\n",
    "    {'title': 'VIX spikes as geopolitical tensions escalate in Middle East', 'source': 'CNBC', 'ticker': 'SPY'},\n",
    "    {'title': 'Retail investors pour record amounts into tech ETFs', 'source': 'MarketWatch', 'ticker': 'QQQ'},\n",
    "    {'title': 'Insider selling at tech companies reaches 2-year high', 'source': 'Seeking Alpha', 'ticker': 'QQQ'},\n",
    "    {'title': 'Bitcoin ETF inflows hit $1B in single day, boosting crypto stocks', 'source': 'CNBC', 'ticker': 'SPY'},\n",
    "    {'title': 'Oil prices surge on OPEC production cut extension', 'source': 'Reuters', 'ticker': 'SPY'},\n",
    "    {'title': 'Bank of Japan raises interest rates, yen strengthens sharply', 'source': 'Reuters', 'ticker': 'SPY'},\n",
    "    {'title': 'US consumer confidence falls to 6-month low', 'source': 'MarketWatch', 'ticker': 'SPY'},\n",
    "    {'title': 'Housing starts plummet as mortgage rates stay elevated', 'source': 'CNBC', 'ticker': 'SPY'},\n",
    "]\n",
    "\n",
    "if len(combined_news) < 20:\n",
    "    print(\"Supplementing with simulated headlines for demonstration...\")\n",
    "    for h in SIMULATED_HEADLINES:\n",
    "        h.setdefault('summary', '')\n",
    "        h.setdefault('published', '')\n",
    "        h.setdefault('link', '')\n",
    "    combined_news.extend(SIMULATED_HEADLINES)\n",
    "\n",
    "news_df = pd.DataFrame(combined_news)\n",
    "# Clean: drop rows with empty titles\n",
    "news_df = news_df[news_df['title'].str.strip().str.len() > 0].reset_index(drop=True)\n",
    "print(f\"\\nTotal headlines for analysis: {len(news_df)}\")\n",
    "print(f\"Sources: {news_df['source'].nunique()} unique sources\")\n",
    "news_df[['source', 'title']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Multi-Model Sentiment Analysis\n",
    "\n",
    "Different models have different strengths. We'll compare three:\n",
    "\n",
    "| Model | Strength | Speed |\n",
    "|-------|----------|-------|\n",
    "| **FinBERT** | Best for formal financial text (news, filings) | Medium |\n",
    "| **Twitter-RoBERTa** | Best for social media / informal text | Medium |\n",
    "| **DistilRoBERTa-Financial** | Fast alternative, good for real-time | Fast |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "print(\"Loading sentiment models onto GPU...\")\n",
    "print(\"(First run downloads models; subsequent runs use cache)\\n\")\n",
    "\n",
    "# Model 1: FinBERT - the gold standard for financial sentiment\n",
    "print(\"[1/3] Loading FinBERT...\")\n",
    "finbert = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"ProsusAI/finbert\",\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")\n",
    "print(\"  FinBERT loaded.\")\n",
    "\n",
    "# Model 2: Twitter-RoBERTa - great for social media sentiment\n",
    "print(\"[2/3] Loading Twitter-RoBERTa...\")\n",
    "twitter_sentiment = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")\n",
    "print(\"  Twitter-RoBERTa loaded.\")\n",
    "\n",
    "# Model 3: DistilRoBERTa fine-tuned on financial news\n",
    "print(\"[3/3] Loading DistilRoBERTa-Financial...\")\n",
    "distil_financial = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\",\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")\n",
    "print(\"  DistilRoBERTa-Financial loaded.\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    mem_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    mem_total = torch.cuda.get_device_properties(0).total_mem / 1024**3\n",
    "    print(f\"\\nGPU Memory: {mem_used:.1f} GB used / {mem_total:.1f} GB total\")\n",
    "    print(f\"All 3 models loaded simultaneously -- plenty of room for more.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentiment(result, model_name):\n",
    "    \"\"\"\n",
    "    Convert model-specific labels to a unified score from -1 (bearish) to +1 (bullish).\n",
    "    \n",
    "    Different models use different label names:\n",
    "    - FinBERT: positive / negative / neutral\n",
    "    - Twitter-RoBERTa: positive / negative / neutral  \n",
    "    - DistilRoBERTa: positive / negative / neutral\n",
    "    \"\"\"\n",
    "    label = result['label'].lower()\n",
    "    score = result['score']\n",
    "    \n",
    "    if 'positive' in label or label == 'pos':\n",
    "        return score  # +0.0 to +1.0\n",
    "    elif 'negative' in label or label == 'neg':\n",
    "        return -score  # -1.0 to -0.0\n",
    "    else:  # neutral\n",
    "        return 0.0\n",
    "\n",
    "# Test with sample headlines\n",
    "test_headlines = [\n",
    "    \"NVIDIA beats earnings expectations by 20%, stock surges in after-hours\",\n",
    "    \"Tesla recalls 500,000 vehicles over brake safety concerns\",\n",
    "    \"Federal Reserve keeps interest rates unchanged as expected\",\n",
    "    \"Apple announces massive $110 billion share buyback program\",\n",
    "    \"Semiconductor stocks plunge on new China export restrictions\",\n",
    "]\n",
    "\n",
    "print(\"Comparing 3 models on sample headlines:\\n\")\n",
    "print(f\"{'Headline':<60} {'FinBERT':>8} {'Twitter':>8} {'Distil':>8} {'Avg':>8}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for headline in test_headlines:\n",
    "    fb = normalize_sentiment(finbert(headline)[0], 'finbert')\n",
    "    tw = normalize_sentiment(twitter_sentiment(headline)[0], 'twitter')\n",
    "    dr = normalize_sentiment(distil_financial(headline)[0], 'distil')\n",
    "    avg = (fb + tw + dr) / 3\n",
    "    \n",
    "    short_headline = headline[:58] + '...' if len(headline) > 58 else headline\n",
    "    print(f\"{short_headline:<60} {fb:>+8.3f} {tw:>+8.3f} {dr:>+8.3f} {avg:>+8.3f}\")\n",
    "\n",
    "print(\"\\nScore range: -1.0 (very bearish) to +1.0 (very bullish), 0.0 = neutral\")\n",
    "print(\"Averaging across models reduces noise from any single model's biases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Entity Extraction - Which Stock is This About?\n",
    "\n",
    "For general news (not stock-specific feeds), we need to figure out which\n",
    "company/ticker a headline refers to. We'll use two approaches:\n",
    "\n",
    "1. **Keyword matching** - Fast, simple, catches most cases\n",
    "2. **NER model** - Catches company names that keyword matching misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Keyword / Ticker matching\n",
    "# This is fast and catches the majority of cases\n",
    "\n",
    "TICKER_MAP = {\n",
    "    # Ticker symbols\n",
    "    'NVDA': 'NVDA', 'AAPL': 'AAPL', 'TSLA': 'TSLA', 'AMD': 'AMD',\n",
    "    'META': 'META', 'MSFT': 'MSFT', 'AMZN': 'AMZN', 'GOOGL': 'GOOGL',\n",
    "    'GOOG': 'GOOGL', 'SPY': 'SPY', 'QQQ': 'QQQ',\n",
    "    # Company names -> tickers\n",
    "    'NVIDIA': 'NVDA', 'Nvidia': 'NVDA',\n",
    "    'Apple': 'AAPL', 'iPhone': 'AAPL', 'iPad': 'AAPL', 'MacBook': 'AAPL',\n",
    "    'Tesla': 'TSLA', 'Cybertruck': 'TSLA', 'Musk': 'TSLA',\n",
    "    'Microsoft': 'MSFT', 'Azure': 'MSFT', 'Copilot': 'MSFT',\n",
    "    'Amazon': 'AMZN', 'AWS': 'AMZN',\n",
    "    'Google': 'GOOGL', 'Alphabet': 'GOOGL', 'YouTube': 'GOOGL',\n",
    "    'Meta': 'META', 'Facebook': 'META', 'Instagram': 'META', 'Reels': 'META',\n",
    "    'S&P 500': 'SPY', 'S&P': 'SPY',\n",
    "    'Nasdaq': 'QQQ',\n",
    "}\n",
    "\n",
    "def extract_tickers_keyword(text):\n",
    "    \"\"\"Extract ticker symbols from text using keyword matching.\"\"\"\n",
    "    found = set()\n",
    "    for keyword, ticker in TICKER_MAP.items():\n",
    "        # Use word boundary matching for short tickers to avoid false positives\n",
    "        if len(keyword) <= 4:\n",
    "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', text):\n",
    "                found.add(ticker)\n",
    "        else:\n",
    "            if keyword in text:\n",
    "                found.add(ticker)\n",
    "    return list(found)\n",
    "\n",
    "# Test it\n",
    "test_texts = [\n",
    "    \"NVIDIA beats Q4 earnings estimates, data center revenue surges\",\n",
    "    \"Apple and Microsoft both report strong cloud revenue growth\",\n",
    "    \"Elon Musk announces Tesla robotaxi launch date\",\n",
    "    \"S&P 500 hits new all-time high on strong jobs data\",\n",
    "    \"Tech stocks lead market rally as Nasdaq surges 2%\",\n",
    "]\n",
    "\n",
    "print(\"Keyword-based ticker extraction:\\n\")\n",
    "for text in test_texts:\n",
    "    tickers = extract_tickers_keyword(text)\n",
    "    print(f\"  {tickers!s:<20} <- {text[:65]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2: NER (Named Entity Recognition) for catching company names\n",
    "# This catches cases like \"the Cupertino-based tech giant\" -> Apple\n",
    "\n",
    "print(\"Loading NER model...\")\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    aggregation_strategy=\"simple\",\n",
    ")\n",
    "print(\"NER model loaded.\\n\")\n",
    "\n",
    "# Known organization -> ticker mapping for NER results\n",
    "ORG_TO_TICKER = {\n",
    "    'nvidia': 'NVDA', 'apple': 'AAPL', 'tesla': 'TSLA',\n",
    "    'amd': 'AMD', 'advanced micro devices': 'AMD',\n",
    "    'meta': 'META', 'meta platforms': 'META', 'facebook': 'META',\n",
    "    'microsoft': 'MSFT', 'amazon': 'AMZN', 'google': 'GOOGL',\n",
    "    'alphabet': 'GOOGL',\n",
    "}\n",
    "\n",
    "def extract_tickers_ner(text):\n",
    "    \"\"\"Extract tickers using NER model to find organization names.\"\"\"\n",
    "    entities = ner_pipeline(text)\n",
    "    found = set()\n",
    "    for ent in entities:\n",
    "        if ent['entity_group'] == 'ORG':\n",
    "            org_name = ent['word'].lower().strip()\n",
    "            if org_name in ORG_TO_TICKER:\n",
    "                found.add(ORG_TO_TICKER[org_name])\n",
    "    return list(found)\n",
    "\n",
    "def extract_tickers_combined(text):\n",
    "    \"\"\"Combine keyword and NER approaches for best coverage.\"\"\"\n",
    "    keyword_tickers = set(extract_tickers_keyword(text))\n",
    "    ner_tickers = set(extract_tickers_ner(text))\n",
    "    return list(keyword_tickers | ner_tickers)\n",
    "\n",
    "# Compare approaches\n",
    "print(f\"{'Text':<55} {'Keyword':>14} {'NER':>14} {'Combined':>14}\")\n",
    "print(\"-\" * 100)\n",
    "for text in test_texts:\n",
    "    kw = extract_tickers_keyword(text)\n",
    "    ner = extract_tickers_ner(text)\n",
    "    combined = extract_tickers_combined(text)\n",
    "    short = text[:53] + '...' if len(text) > 53 else text\n",
    "    print(f\"{short:<55} {str(kw):>14} {str(ner):>14} {str(combined):>14}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Full Sentiment Pipeline - Process All Headlines\n",
    "\n",
    "Now we combine everything into a single pipeline:\n",
    "**Headlines -> Entity Extraction -> Multi-Model Sentiment -> Aggregated Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentPipeline:\n",
    "    \"\"\"GPU-accelerated financial sentiment analysis pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, finbert_model, twitter_model, distil_model, batch_size=32):\n",
    "        self.finbert = finbert_model\n",
    "        self.twitter = twitter_model\n",
    "        self.distil = distil_model\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def analyze_batch(self, texts):\n",
    "        \"\"\"\n",
    "        Analyze a batch of texts with all three models.\n",
    "        Returns list of dicts with normalized scores.\n",
    "        \"\"\"\n",
    "        # Run all three models on the batch\n",
    "        fb_results = self.finbert(texts, batch_size=self.batch_size)\n",
    "        tw_results = self.twitter(texts, batch_size=self.batch_size)\n",
    "        dr_results = self.distil(texts, batch_size=self.batch_size)\n",
    "        \n",
    "        results = []\n",
    "        for fb, tw, dr in zip(fb_results, tw_results, dr_results):\n",
    "            fb_score = normalize_sentiment(fb, 'finbert')\n",
    "            tw_score = normalize_sentiment(tw, 'twitter')\n",
    "            dr_score = normalize_sentiment(dr, 'distil')\n",
    "            ensemble_score = (fb_score + tw_score + dr_score) / 3\n",
    "            \n",
    "            results.append({\n",
    "                'finbert_score': fb_score,\n",
    "                'twitter_score': tw_score,\n",
    "                'distil_score': dr_score,\n",
    "                'ensemble_score': ensemble_score,\n",
    "                'finbert_label': fb['label'],\n",
    "                'twitter_label': tw['label'],\n",
    "                'distil_label': dr['label'],\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def process_headlines(self, df, text_column='title'):\n",
    "        \"\"\"\n",
    "        Process a DataFrame of headlines through the full pipeline.\n",
    "        Adds sentiment scores and extracted tickers.\n",
    "        \"\"\"\n",
    "        texts = df[text_column].tolist()\n",
    "        \n",
    "        # Sentiment analysis (GPU-accelerated)\n",
    "        sentiment_results = self.analyze_batch(texts)\n",
    "        \n",
    "        # Entity extraction (keyword-based for speed)\n",
    "        ticker_results = [extract_tickers_keyword(t) for t in texts]\n",
    "        \n",
    "        # Add results to DataFrame\n",
    "        result_df = df.copy()\n",
    "        for key in ['finbert_score', 'twitter_score', 'distil_score', 'ensemble_score',\n",
    "                     'finbert_label', 'twitter_label', 'distil_label']:\n",
    "            result_df[key] = [r[key] for r in sentiment_results]\n",
    "        \n",
    "        result_df['extracted_tickers'] = ticker_results\n",
    "        \n",
    "        # Use extracted tickers if no ticker column exists, otherwise keep existing\n",
    "        if 'ticker' not in result_df.columns:\n",
    "            result_df['ticker'] = result_df['extracted_tickers']\n",
    "        else:\n",
    "            # Fill NaN tickers with extracted ones\n",
    "            mask = result_df['ticker'].isna() | (result_df['ticker'] == '')\n",
    "            result_df.loc[mask, 'ticker'] = result_df.loc[mask, 'extracted_tickers']\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline_instance = SentimentPipeline(finbert, twitter_sentiment, distil_financial)\n",
    "print(\"Sentiment pipeline ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all headlines through the pipeline\n",
    "print(f\"Processing {len(news_df)} headlines through 3 sentiment models on GPU...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "results_df = pipeline_instance.process_headlines(news_df)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"Done in {elapsed:.2f}s ({len(news_df)/elapsed:.0f} headlines/sec across 3 models)\")\n",
    "print(f\"That's effectively {len(news_df)*3/elapsed:.0f} individual model inferences/sec\\n\")\n",
    "\n",
    "# Show results\n",
    "display_cols = ['source', 'title', 'ensemble_score', 'finbert_score', 'twitter_score', 'distil_score']\n",
    "results_df[display_cols].head(10).style.background_gradient(\n",
    "    subset=['ensemble_score', 'finbert_score', 'twitter_score', 'distil_score'],\n",
    "    cmap='RdYlGn', vmin=-1, vmax=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of sentiment scores\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "for ax, col, name in zip(axes, \n",
    "    ['finbert_score', 'twitter_score', 'distil_score', 'ensemble_score'],\n",
    "    ['FinBERT', 'Twitter-RoBERTa', 'DistilRoBERTa', 'Ensemble (Average)']):\n",
    "    \n",
    "    ax.hist(results_df[col], bins=30, color='cyan', alpha=0.7, edgecolor='white', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='yellow', linestyle='--', alpha=0.5)\n",
    "    ax.set_title(name, fontsize=10)\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_xlim(-1.1, 1.1)\n",
    "\n",
    "axes[0].set_ylabel('Count')\n",
    "plt.suptitle('Sentiment Score Distributions Across Models', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how different models have different biases:\")\n",
    "print(f\"  FinBERT avg:    {results_df['finbert_score'].mean():+.3f}\")\n",
    "print(f\"  Twitter avg:    {results_df['twitter_score'].mean():+.3f}\")\n",
    "print(f\"  DistilRoBERTa:  {results_df['distil_score'].mean():+.3f}\")\n",
    "print(f\"  Ensemble avg:   {results_df['ensemble_score'].mean():+.3f}\")\n",
    "print(\"\\nThe ensemble average smooths out individual model biases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model agreement analysis - when do models disagree?\n",
    "# Disagreement can itself be a signal (ambiguous news = uncertainty)\n",
    "\n",
    "results_df['model_std'] = results_df[['finbert_score', 'twitter_score', 'distil_score']].std(axis=1)\n",
    "\n",
    "# Headlines where models strongly agree vs disagree\n",
    "print(\"=== Headlines Where Models AGREE (low std) ===\")\n",
    "agree = results_df.nsmallest(5, 'model_std')\n",
    "for _, row in agree.iterrows():\n",
    "    print(f\"  Ensemble: {row['ensemble_score']:+.3f} (std: {row['model_std']:.3f})\")\n",
    "    print(f\"  -> {row['title'][:80]}\\n\")\n",
    "\n",
    "print(\"\\n=== Headlines Where Models DISAGREE (high std) ===\")\n",
    "disagree = results_df.nlargest(5, 'model_std')\n",
    "for _, row in disagree.iterrows():\n",
    "    print(f\"  FB: {row['finbert_score']:+.3f} | TW: {row['twitter_score']:+.3f} | DR: {row['distil_score']:+.3f} (std: {row['model_std']:.3f})\")\n",
    "    print(f\"  -> {row['title'][:80]}\\n\")\n",
    "\n",
    "print(\"High disagreement often means the headline is ambiguous or contains mixed signals.\")\n",
    "print(\"This uncertainty metric is itself useful -- trade with caution on ambiguous news.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Per-Stock Sentiment Aggregation\n",
    "\n",
    "For day trading, we want to know: **\"What is the overall sentiment for NVDA right now?\"**\n",
    "\n",
    "We aggregate headline-level scores into stock-level sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_sentiment_by_ticker(df):\n",
    "    \"\"\"\n",
    "    Explode multi-ticker headlines and aggregate sentiment per stock.\n",
    "    \n",
    "    A headline mentioning both NVDA and AMD contributes to both stocks' scores.\n",
    "    \"\"\"\n",
    "    # Handle both list and string ticker columns\n",
    "    expanded_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        tickers = row.get('extracted_tickers', [])\n",
    "        if isinstance(tickers, str):\n",
    "            tickers = [tickers] if tickers else []\n",
    "        \n",
    "        # Also include the original ticker if available\n",
    "        original = row.get('ticker', None)\n",
    "        if isinstance(original, str) and original:\n",
    "            tickers = list(set(tickers + [original]))\n",
    "        elif isinstance(original, list):\n",
    "            tickers = list(set(tickers + original))\n",
    "        \n",
    "        for ticker in tickers:\n",
    "            if ticker and isinstance(ticker, str):\n",
    "                new_row = row.copy()\n",
    "                new_row['resolved_ticker'] = ticker\n",
    "                expanded_rows.append(new_row)\n",
    "    \n",
    "    if not expanded_rows:\n",
    "        print(\"No tickers found in headlines.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    expanded_df = pd.DataFrame(expanded_rows)\n",
    "    \n",
    "    # Aggregate per ticker\n",
    "    agg = expanded_df.groupby('resolved_ticker').agg(\n",
    "        headline_count=('ensemble_score', 'count'),\n",
    "        avg_sentiment=('ensemble_score', 'mean'),\n",
    "        sentiment_std=('ensemble_score', 'std'),\n",
    "        max_sentiment=('ensemble_score', 'max'),\n",
    "        min_sentiment=('ensemble_score', 'min'),\n",
    "        finbert_avg=('finbert_score', 'mean'),\n",
    "        twitter_avg=('twitter_score', 'mean'),\n",
    "        distil_avg=('distil_score', 'mean'),\n",
    "    ).round(3)\n",
    "    \n",
    "    agg['sentiment_range'] = agg['max_sentiment'] - agg['min_sentiment']\n",
    "    \n",
    "    return agg.sort_values('avg_sentiment', ascending=False), expanded_df\n",
    "\n",
    "ticker_sentiment, expanded_df = aggregate_sentiment_by_ticker(results_df)\n",
    "\n",
    "print(\"=== Per-Stock Sentiment Scoreboard ===\\n\")\n",
    "print(ticker_sentiment[['headline_count', 'avg_sentiment', 'sentiment_std', 'min_sentiment', 'max_sentiment']].to_string())\n",
    "print(\"\\nPositive avg_sentiment = bullish news flow\")\n",
    "print(\"Negative avg_sentiment = bearish news flow\")\n",
    "print(\"High sentiment_std = mixed/conflicting news (be cautious)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-stock sentiment\n",
    "if not ticker_sentiment.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Bar chart of average sentiment\n",
    "    ax1 = axes[0]\n",
    "    colors = ['green' if s > 0.05 else 'red' if s < -0.05 else 'gray' \n",
    "              for s in ticker_sentiment['avg_sentiment']]\n",
    "    bars = ax1.barh(ticker_sentiment.index, ticker_sentiment['avg_sentiment'], color=colors, alpha=0.8)\n",
    "    ax1.axvline(x=0, color='white', linestyle='-', alpha=0.3)\n",
    "    ax1.set_xlabel('Average Ensemble Sentiment')\n",
    "    ax1.set_title('Sentiment by Stock (Bullish > 0 > Bearish)')\n",
    "    \n",
    "    # Add headline count annotations\n",
    "    for i, (idx, row) in enumerate(ticker_sentiment.iterrows()):\n",
    "        ax1.text(row['avg_sentiment'] + 0.02 * np.sign(row['avg_sentiment']), i,\n",
    "                f\"n={int(row['headline_count'])}\", va='center', fontsize=8, color='white')\n",
    "\n",
    "    # Sentiment range (uncertainty) by stock\n",
    "    ax2 = axes[1]\n",
    "    for i, (ticker, row) in enumerate(ticker_sentiment.iterrows()):\n",
    "        ax2.plot([row['min_sentiment'], row['max_sentiment']], [i, i], \n",
    "                color='cyan', linewidth=3, alpha=0.7)\n",
    "        ax2.plot(row['avg_sentiment'], i, 'o', color='yellow', markersize=8)\n",
    "    \n",
    "    ax2.set_yticks(range(len(ticker_sentiment)))\n",
    "    ax2.set_yticklabels(ticker_sentiment.index)\n",
    "    ax2.axvline(x=0, color='white', linestyle='-', alpha=0.3)\n",
    "    ax2.set_xlabel('Sentiment Score Range')\n",
    "    ax2.set_title('Sentiment Range (line) & Average (dot)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Wide ranges indicate conflicting headlines -- higher uncertainty.\")\n",
    "    print(\"Narrow ranges with strong sentiment = clearer directional signal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drill down: show individual headlines for a specific stock\n",
    "def show_stock_headlines(expanded_df, ticker, n=10):\n",
    "    \"\"\"Show the most bullish and bearish headlines for a given stock.\"\"\"\n",
    "    stock_df = expanded_df[expanded_df['resolved_ticker'] == ticker].copy()\n",
    "    if stock_df.empty:\n",
    "        print(f\"No headlines found for {ticker}\")\n",
    "        return\n",
    "    \n",
    "    stock_df = stock_df.sort_values('ensemble_score', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  {ticker} SENTIMENT BREAKDOWN ({len(stock_df)} headlines)\")\n",
    "    print(f\"  Average: {stock_df['ensemble_score'].mean():+.3f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    top_n = min(n // 2, len(stock_df))\n",
    "    \n",
    "    print(f\"\\n  Most BULLISH:\")\n",
    "    for _, row in stock_df.head(top_n).iterrows():\n",
    "        print(f\"    {row['ensemble_score']:+.3f} | {row['title'][:65]}\")\n",
    "    \n",
    "    print(f\"\\n  Most BEARISH:\")\n",
    "    for _, row in stock_df.tail(top_n).iterrows():\n",
    "        print(f\"    {row['ensemble_score']:+.3f} | {row['title'][:65]}\")\n",
    "\n",
    "# Show breakdowns for top stocks\n",
    "for ticker in ['NVDA', 'TSLA', 'AAPL']:\n",
    "    show_stock_headlines(expanded_df, ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Sentiment-Price Correlation\n",
    "\n",
    "The big question: **Does news sentiment actually predict price moves?**\n",
    "\n",
    "We'll correlate historical news sentiment with actual price changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a historical sentiment dataset using Yahoo Finance news + prices\n",
    "# We'll analyze NVDA as our case study\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "# Get price data\n",
    "nvda_prices = yf.download('NVDA', period='6mo', interval='1d', progress=False)\n",
    "if isinstance(nvda_prices.columns, pd.MultiIndex):\n",
    "    nvda_prices.columns = nvda_prices.columns.get_level_values(0)\n",
    "\n",
    "# Calculate daily returns\n",
    "nvda_prices['Return'] = nvda_prices['Close'].pct_change()\n",
    "nvda_prices['Return_Next'] = nvda_prices['Return'].shift(-1)  # Next day's return\n",
    "nvda_prices['Abs_Return'] = nvda_prices['Return'].abs()\n",
    "\n",
    "# Calculate a simple \"news impact\" metric\n",
    "# Large absolute returns often correspond to major news events\n",
    "print(\"NVDA Daily Return Statistics:\")\n",
    "print(f\"  Mean daily return: {nvda_prices['Return'].mean()*100:+.2f}%\")\n",
    "print(f\"  Std deviation:     {nvda_prices['Return'].std()*100:.2f}%\")\n",
    "print(f\"  Biggest up day:    {nvda_prices['Return'].max()*100:+.2f}%\")\n",
    "print(f\"  Biggest down day:  {nvda_prices['Return'].min()*100:+.2f}%\")\n",
    "\n",
    "# Days with >2 std dev moves (likely news-driven)\n",
    "threshold = nvda_prices['Return'].std() * 2\n",
    "big_move_days = nvda_prices[nvda_prices['Abs_Return'] > threshold]\n",
    "print(f\"\\n  Days with >2 sigma moves: {len(big_move_days)} out of {len(nvda_prices)}\")\n",
    "print(f\"  These are likely news-driven events.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a historical sentiment signal to demonstrate the correlation analysis\n",
    "# In production, you'd store sentiment scores daily from your live pipeline\n",
    "\n",
    "np.random.seed(42)\n",
    "n_days = len(nvda_prices)\n",
    "\n",
    "# Create a synthetic sentiment signal that has some correlation with returns\n",
    "# This simulates what you'd build with a real sentiment feed\n",
    "noise = np.random.normal(0, 0.3, n_days)\n",
    "# Sentiment partially correlates with same-day returns (news moves prices)\n",
    "# and partially with previous day's returns (momentum/narrative follows price)\n",
    "returns = nvda_prices['Return'].fillna(0).values\n",
    "lagged_returns = np.roll(returns, 1)\n",
    "lagged_returns[0] = 0\n",
    "\n",
    "synthetic_sentiment = 0.4 * np.sign(returns) * np.abs(returns) * 20 + \\\n",
    "                      0.2 * np.sign(lagged_returns) * np.abs(lagged_returns) * 15 + \\\n",
    "                      noise\n",
    "synthetic_sentiment = np.clip(synthetic_sentiment, -1, 1)\n",
    "\n",
    "nvda_prices['Sentiment'] = synthetic_sentiment\n",
    "\n",
    "# Visualize the relationship\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12), gridspec_kw={'height_ratios': [2, 1, 1]})\n",
    "\n",
    "# Price\n",
    "ax1 = axes[0]\n",
    "ax1.plot(nvda_prices.index, nvda_prices['Close'], color='white', linewidth=1.5)\n",
    "ax1.set_ylabel('Price ($)')\n",
    "ax1.set_title('NVDA Price, Sentiment Signal, and Returns')\n",
    "\n",
    "# Sentiment\n",
    "ax2 = axes[1]\n",
    "colors = ['green' if s > 0 else 'red' for s in nvda_prices['Sentiment']]\n",
    "ax2.bar(nvda_prices.index, nvda_prices['Sentiment'], color=colors, alpha=0.7)\n",
    "ax2.set_ylabel('Sentiment')\n",
    "ax2.axhline(y=0, color='white', linestyle='-', alpha=0.3)\n",
    "ax2.set_ylim(-1.1, 1.1)\n",
    "ax2.set_title('Daily Sentiment Score')\n",
    "\n",
    "# Daily returns\n",
    "ax3 = axes[2]\n",
    "ret_colors = ['green' if r > 0 else 'red' for r in nvda_prices['Return'].fillna(0)]\n",
    "ax3.bar(nvda_prices.index, nvda_prices['Return'].fillna(0) * 100, color=ret_colors, alpha=0.7)\n",
    "ax3.set_ylabel('Return (%)')\n",
    "ax3.axhline(y=0, color='white', linestyle='-', alpha=0.3)\n",
    "ax3.set_title('Daily Returns (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Sentiment vs Same-Day Return\n",
    "ax1 = axes[0]\n",
    "valid = nvda_prices.dropna(subset=['Return', 'Sentiment'])\n",
    "ax1.scatter(valid['Sentiment'], valid['Return'] * 100, alpha=0.5, s=20, color='cyan')\n",
    "z = np.polyfit(valid['Sentiment'], valid['Return'] * 100, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(-1, 1, 100)\n",
    "ax1.plot(x_line, p(x_line), color='yellow', linewidth=2, linestyle='--')\n",
    "corr = valid['Sentiment'].corr(valid['Return'])\n",
    "ax1.set_title(f'Sentiment vs Same-Day Return\\nr = {corr:.3f}')\n",
    "ax1.set_xlabel('Sentiment Score')\n",
    "ax1.set_ylabel('Daily Return (%)')\n",
    "ax1.axhline(y=0, color='white', alpha=0.2)\n",
    "ax1.axvline(x=0, color='white', alpha=0.2)\n",
    "\n",
    "# Sentiment vs Next-Day Return (predictive power)\n",
    "ax2 = axes[1]\n",
    "valid2 = nvda_prices.dropna(subset=['Return_Next', 'Sentiment'])\n",
    "ax2.scatter(valid2['Sentiment'], valid2['Return_Next'] * 100, alpha=0.5, s=20, color='cyan')\n",
    "z2 = np.polyfit(valid2['Sentiment'], valid2['Return_Next'] * 100, 1)\n",
    "p2 = np.poly1d(z2)\n",
    "ax2.plot(x_line, p2(x_line), color='yellow', linewidth=2, linestyle='--')\n",
    "corr2 = valid2['Sentiment'].corr(valid2['Return_Next'])\n",
    "ax2.set_title(f'Sentiment vs NEXT-Day Return\\nr = {corr2:.3f}')\n",
    "ax2.set_xlabel('Sentiment Score')\n",
    "ax2.set_ylabel('Next-Day Return (%)')\n",
    "ax2.axhline(y=0, color='white', alpha=0.2)\n",
    "ax2.axvline(x=0, color='white', alpha=0.2)\n",
    "\n",
    "# Sentiment quintile analysis\n",
    "ax3 = axes[2]\n",
    "valid3 = nvda_prices.dropna(subset=['Return_Next', 'Sentiment']).copy()\n",
    "valid3['Sentiment_Quintile'] = pd.qcut(valid3['Sentiment'], q=5, labels=['Very Bearish', 'Bearish', 'Neutral', 'Bullish', 'Very Bullish'])\n",
    "quintile_returns = valid3.groupby('Sentiment_Quintile', observed=True)['Return_Next'].mean() * 100\n",
    "colors = ['#ff4444', '#ff8888', 'gray', '#88ff88', '#44ff44']\n",
    "bars = ax3.bar(range(len(quintile_returns)), quintile_returns.values, color=colors, alpha=0.8)\n",
    "ax3.set_xticks(range(len(quintile_returns)))\n",
    "ax3.set_xticklabels(quintile_returns.index, rotation=30, ha='right', fontsize=8)\n",
    "ax3.set_ylabel('Avg Next-Day Return (%)')\n",
    "ax3.set_title('Next-Day Return by Sentiment Quintile')\n",
    "ax3.axhline(y=0, color='white', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Questions to Investigate:\")\n",
    "print(\"  1. Does extreme sentiment (very bullish/bearish) predict next-day moves?\")\n",
    "print(\"  2. Does sentiment have more predictive power on high-volume days?\")\n",
    "print(\"  3. Is the signal stronger for certain stocks or sectors?\")\n",
    "print(\"  4. Does the signal decay quickly (minutes) or persist (days)?\")\n",
    "print(\"\\nWith your live pipeline, you can answer these questions with REAL data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. GPU Throughput Benchmark\n",
    "\n",
    "Let's measure exactly how fast your 4090 can process financial text.\n",
    "This determines the scale of real-time analysis you can support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: How many headlines can we process per second?\n",
    "\n",
    "# Create a large batch of varied financial headlines\n",
    "benchmark_headlines = [\n",
    "    \"NVIDIA reports record quarterly revenue of $22.1 billion\",\n",
    "    \"Federal Reserve raises interest rates by 25 basis points\",\n",
    "    \"Tesla deliveries miss Wall Street expectations for Q3\",\n",
    "    \"Apple announces $90 billion share buyback program\",\n",
    "    \"Amazon Web Services growth accelerates to 17% year-over-year\",\n",
    "    \"Microsoft cloud revenue beats estimates driven by AI demand\",\n",
    "    \"Meta platforms cuts 10,000 jobs in latest round of layoffs\",\n",
    "    \"Google faces $2.3 billion antitrust fine from European regulators\",\n",
    "    \"AMD gains market share in server CPUs for fifth consecutive quarter\",\n",
    "    \"Oil prices drop 5% as OPEC increases production quotas\",\n",
    "] * 100  # 1000 headlines\n",
    "\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "results_benchmark = {}\n",
    "\n",
    "print(f\"Benchmarking FinBERT on {len(benchmark_headlines)} headlines...\\n\")\n",
    "print(f\"{'Batch Size':>12} {'Time (s)':>10} {'Headlines/sec':>15} {'Speedup vs BS=8':>18}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "baseline_speed = None\n",
    "for bs in batch_sizes:\n",
    "    # Warm up\n",
    "    _ = finbert(benchmark_headlines[:bs], batch_size=bs)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Time it\n",
    "    start = time.time()\n",
    "    _ = finbert(benchmark_headlines, batch_size=bs)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    speed = len(benchmark_headlines) / elapsed\n",
    "    if baseline_speed is None:\n",
    "        baseline_speed = speed\n",
    "    \n",
    "    results_benchmark[bs] = {'time': elapsed, 'speed': speed}\n",
    "    print(f\"{bs:>12} {elapsed:>10.2f} {speed:>15.0f} {speed/baseline_speed:>17.1f}x\")\n",
    "\n",
    "best_bs = max(results_benchmark, key=lambda k: results_benchmark[k]['speed'])\n",
    "best_speed = results_benchmark[best_bs]['speed']\n",
    "print(f\"\\nOptimal batch size: {best_bs}\")\n",
    "print(f\"Peak throughput: {best_speed:.0f} headlines/sec (single model)\")\n",
    "print(f\"\\nAt this rate, you can analyze:\")\n",
    "print(f\"  - {best_speed * 60:,.0f} headlines per minute\")\n",
    "print(f\"  - {best_speed * 3600:,.0f} headlines per hour\")\n",
    "print(f\"  - The entire daily output of major news wires in seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize throughput by batch size\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "bs_list = list(results_benchmark.keys())\n",
    "speeds = [results_benchmark[bs]['speed'] for bs in bs_list]\n",
    "times = [results_benchmark[bs]['time'] for bs in bs_list]\n",
    "\n",
    "ax1.bar(range(len(bs_list)), speeds, color='cyan', alpha=0.8)\n",
    "ax1.set_xticks(range(len(bs_list)))\n",
    "ax1.set_xticklabels([str(bs) for bs in bs_list])\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('Headlines / Second')\n",
    "ax1.set_title('Throughput by Batch Size')\n",
    "\n",
    "ax2.bar(range(len(bs_list)), times, color='orange', alpha=0.8)\n",
    "ax2.set_xticks(range(len(bs_list)))\n",
    "ax2.set_xticklabels([str(bs) for bs in bs_list])\n",
    "ax2.set_xlabel('Batch Size')\n",
    "ax2.set_ylabel('Total Time (seconds)')\n",
    "ax2.set_title(f'Time to Process {len(benchmark_headlines)} Headlines')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Larger batch sizes use GPU parallelism more efficiently.\")\n",
    "print(\"For real-time use, accumulate headlines into mini-batches before processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Live Monitoring Architecture\n",
    "\n",
    "Here's how to build a real-time sentiment monitoring system.\n",
    "This section provides the architecture -- you'd run this as a separate Python script,\n",
    "not in a notebook.\n",
    "\n",
    "```\n",
    "            \n",
    "  News Sources      Headline          GPU Sentiment        Trading     \n",
    "              >  Queue       >  Pipeline       >  Signals     \n",
    "  - RSS feeds                                                          \n",
    "  - NewsAPI         Batch every       - FinBERT            - Per-stock \n",
    "  - Twitter/X       5 seconds         - Twitter-RoBERTa      scores    \n",
    "  - Reddit                            - NER                - Alerts    \n",
    "  - SEC EDGAR                                              - Dashboard \n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture blueprint: Real-time Sentiment Monitor\n",
    "# Save this as a standalone script when ready to deploy\n",
    "\n",
    "MONITOR_CODE = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Real-Time Financial Sentiment Monitor\n",
    "Runs as a background service, continuously analyzing news.\n",
    "\n",
    "Usage:\n",
    "    python sentiment_monitor.py --watchlist NVDA,TSLA,AAPL --interval 30\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import json\n",
    "import threading\n",
    "import queue\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import feedparser\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "class SentimentMonitor:\n",
    "    def __init__(self, watchlist, batch_interval=5, alert_threshold=0.5):\n",
    "        self.watchlist = watchlist\n",
    "        self.batch_interval = batch_interval  # seconds between processing batches\n",
    "        self.alert_threshold = alert_threshold\n",
    "        self.headline_queue = queue.Queue()\n",
    "        self.sentiment_history = defaultdict(list)  # ticker -> [(timestamp, score)]\n",
    "        self.seen_headlines = set()  # dedup\n",
    "        \n",
    "        # Load models\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        self.sentiment_model = pipeline(\n",
    "            \"sentiment-analysis\", model=\"ProsusAI/finbert\",\n",
    "            device=device, truncation=True, max_length=512,\n",
    "        )\n",
    "    \n",
    "    def fetch_loop(self, feeds, interval=30):\n",
    "        \"\"\"Continuously fetch headlines from RSS feeds.\"\"\"\n",
    "        while True:\n",
    "            for name, url in feeds.items():\n",
    "                try:\n",
    "                    feed = feedparser.parse(url)\n",
    "                    for entry in feed.entries[:20]:\n",
    "                        title = entry.get(\"title\", \"\")\n",
    "                        if title and title not in self.seen_headlines:\n",
    "                            self.seen_headlines.add(title)\n",
    "                            self.headline_queue.put({\n",
    "                                \"title\": title,\n",
    "                                \"source\": name,\n",
    "                                \"timestamp\": datetime.now().isoformat(),\n",
    "                            })\n",
    "                except Exception as e:\n",
    "                    print(f\"Feed error ({name}): {e}\")\n",
    "            time.sleep(interval)\n",
    "    \n",
    "    def process_loop(self):\n",
    "        \"\"\"Process queued headlines in batches on GPU.\"\"\"\n",
    "        while True:\n",
    "            batch = []\n",
    "            # Collect headlines for batch_interval seconds\n",
    "            deadline = time.time() + self.batch_interval\n",
    "            while time.time() < deadline:\n",
    "                try:\n",
    "                    item = self.headline_queue.get(timeout=0.5)\n",
    "                    batch.append(item)\n",
    "                except queue.Empty:\n",
    "                    continue\n",
    "            \n",
    "            if not batch:\n",
    "                continue\n",
    "            \n",
    "            # Batch sentiment analysis on GPU\n",
    "            texts = [item[\"title\"] for item in batch]\n",
    "            results = self.sentiment_model(texts, batch_size=32)\n",
    "            \n",
    "            for item, result in zip(batch, results):\n",
    "                score = self._normalize(result)\n",
    "                tickers = self._extract_tickers(item[\"title\"])\n",
    "                \n",
    "                for ticker in tickers:\n",
    "                    self.sentiment_history[ticker].append(\n",
    "                        (item[\"timestamp\"], score)\n",
    "                    )\n",
    "                    \n",
    "                    # Alert on strong sentiment\n",
    "                    if abs(score) > self.alert_threshold:\n",
    "                        direction = \"BULLISH\" if score > 0 else \"BEARISH\"\n",
    "                        print(f\"[ALERT] {direction} {ticker} ({score:+.2f}): {item['title'][:60]}\")\n",
    "    \n",
    "    def _normalize(self, result):\n",
    "        label = result[\"label\"].lower()\n",
    "        score = result[\"score\"]\n",
    "        if \"positive\" in label:\n",
    "            return score\n",
    "        elif \"negative\" in label:\n",
    "            return -score\n",
    "        return 0.0\n",
    "    \n",
    "    def _extract_tickers(self, text):\n",
    "        # Simplified keyword matching (use the full version from notebook)\n",
    "        found = []\n",
    "        for ticker in self.watchlist:\n",
    "            if ticker in text.upper():\n",
    "                found.append(ticker)\n",
    "        return found\n",
    "    \n",
    "    def get_current_sentiment(self, ticker, window_minutes=60):\n",
    "        \"\"\"Get average sentiment for a ticker over the last N minutes.\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(minutes=window_minutes)\n",
    "        recent = [\n",
    "            score for ts, score in self.sentiment_history[ticker]\n",
    "            if datetime.fromisoformat(ts) > cutoff\n",
    "        ]\n",
    "        if not recent:\n",
    "            return 0.0, 0\n",
    "        return sum(recent) / len(recent), len(recent)\n",
    "    \n",
    "    def run(self, feeds):\n",
    "        \"\"\"Start the monitor with fetch and process threads.\"\"\"\n",
    "        fetch_thread = threading.Thread(\n",
    "            target=self.fetch_loop, args=(feeds, 30), daemon=True\n",
    "        )\n",
    "        process_thread = threading.Thread(\n",
    "            target=self.process_loop, daemon=True\n",
    "        )\n",
    "        \n",
    "        fetch_thread.start()\n",
    "        process_thread.start()\n",
    "        \n",
    "        print(\"Sentiment monitor running. Press Ctrl+C to stop.\")\n",
    "        try:\n",
    "            while True:\n",
    "                time.sleep(60)\n",
    "                # Print periodic summary\n",
    "                print(f\"\\\\n--- Sentiment Summary ({datetime.now().strftime('%H:%M')}) ---\")\n",
    "                for ticker in self.watchlist:\n",
    "                    avg, count = self.get_current_sentiment(ticker)\n",
    "                    if count > 0:\n",
    "                        print(f\"  {ticker}: {avg:+.3f} ({count} headlines in last hour)\")\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Monitor stopped.\")\n",
    "'''\n",
    "\n",
    "# Save the monitor script\n",
    "with open('sentiment_monitor.py', 'w') as f:\n",
    "    f.write(MONITOR_CODE)\n",
    "\n",
    "print(\"Saved: sentiment_monitor.py\")\n",
    "print(\"\\nTo run the live monitor:\")\n",
    "print(\"  python sentiment_monitor.py\")\n",
    "print(\"\\nIt will continuously:\")\n",
    "print(\"  1. Fetch headlines from RSS feeds every 30 seconds\")\n",
    "print(\"  2. Batch-process them through FinBERT on GPU every 5 seconds\")\n",
    "print(\"  3. Extract tickers and aggregate sentiment per stock\")\n",
    "print(\"  4. Alert on strong sentiment signals (>0.5 or <-0.5)\")\n",
    "print(\"  5. Print hourly sentiment summaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Advanced: Sentiment-Based Trading Signals\n",
    "\n",
    "How to convert raw sentiment into actionable trading signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple sentiment-based signal generator\n",
    "\n",
    "class SentimentSignalGenerator:\n",
    "    \"\"\"\n",
    "    Converts raw sentiment scores into trading signals.\n",
    "    \n",
    "    Signal types:\n",
    "      +2 = Strong bullish (high confidence buy signal)\n",
    "      +1 = Mild bullish (lean long)\n",
    "       0 = Neutral (no signal)\n",
    "      -1 = Mild bearish (lean short)\n",
    "      -2 = Strong bearish (high confidence sell signal)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, strong_threshold=0.4, mild_threshold=0.15, \n",
    "                 min_headlines=3, agreement_threshold=0.3):\n",
    "        self.strong_threshold = strong_threshold\n",
    "        self.mild_threshold = mild_threshold\n",
    "        self.min_headlines = min_headlines\n",
    "        self.agreement_threshold = agreement_threshold\n",
    "    \n",
    "    def generate_signal(self, avg_sentiment, headline_count, sentiment_std):\n",
    "        \"\"\"\n",
    "        Generate a trading signal from aggregated sentiment data.\n",
    "        \n",
    "        Parameters:\n",
    "            avg_sentiment: Average ensemble score (-1 to +1)\n",
    "            headline_count: Number of headlines analyzed\n",
    "            sentiment_std: Standard deviation (model agreement)\n",
    "        \"\"\"\n",
    "        # Not enough data\n",
    "        if headline_count < self.min_headlines:\n",
    "            return 0, \"Insufficient data\"\n",
    "        \n",
    "        # High disagreement between models = unreliable\n",
    "        if sentiment_std > self.agreement_threshold:\n",
    "            return 0, \"Models disagree -- ambiguous\"\n",
    "        \n",
    "        # Generate signal based on average sentiment\n",
    "        if avg_sentiment > self.strong_threshold:\n",
    "            return 2, f\"STRONG BULLISH (sentiment: {avg_sentiment:+.3f})\"\n",
    "        elif avg_sentiment > self.mild_threshold:\n",
    "            return 1, f\"Mild bullish (sentiment: {avg_sentiment:+.3f})\"\n",
    "        elif avg_sentiment < -self.strong_threshold:\n",
    "            return -2, f\"STRONG BEARISH (sentiment: {avg_sentiment:+.3f})\"\n",
    "        elif avg_sentiment < -self.mild_threshold:\n",
    "            return -1, f\"Mild bearish (sentiment: {avg_sentiment:+.3f})\"\n",
    "        else:\n",
    "            return 0, f\"Neutral (sentiment: {avg_sentiment:+.3f})\"\n",
    "\n",
    "# Apply to our per-stock sentiment data\n",
    "signal_gen = SentimentSignalGenerator()\n",
    "\n",
    "print(\"=== SENTIMENT-BASED TRADING SIGNALS ===\")\n",
    "print(f\"{'Ticker':<8} {'Headlines':>9} {'Avg Sent':>10} {'Std':>8} {'Signal':>8} {'Reasoning'}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "if not ticker_sentiment.empty:\n",
    "    for ticker, row in ticker_sentiment.iterrows():\n",
    "        signal, reason = signal_gen.generate_signal(\n",
    "            row['avg_sentiment'],\n",
    "            row['headline_count'],\n",
    "            row.get('sentiment_std', 0) if pd.notna(row.get('sentiment_std', 0)) else 0\n",
    "        )\n",
    "        signal_icon = {2: '++', 1: '+ ', 0: '  ', -1: ' -', -2: '--'}[signal]\n",
    "        print(f\"{ticker:<8} {int(row['headline_count']):>9} {row['avg_sentiment']:>+10.3f} \"\n",
    "              f\"{row.get('sentiment_std', 0):>8.3f} {signal_icon:>8} {reason}\")\n",
    "\n",
    "print(\"\\n--- Signal Interpretation ---\")\n",
    "print(\"++ Strong Bullish: Consider long positions with confirmation from technicals\")\n",
    "print(\"+  Mild Bullish:   Lean bullish, but wait for technical entry\")\n",
    "print(\"   Neutral:        No sentiment edge -- rely on technical analysis only\")\n",
    "print(\" - Mild Bearish:   Lean bearish, or tighten stops on longs\")\n",
    "print(\"-- Strong Bearish: Consider short positions or avoid longs\")\n",
    "print(\"\\nIMPORTANT: Sentiment is ONE input. Always combine with:\")\n",
    "print(\"  - Technical analysis (support/resistance, indicators)\")\n",
    "print(\"  - Volume confirmation\")\n",
    "print(\"  - Risk management (position sizing, stop losses)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud of headlines by sentiment\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "if len(results_df) > 5:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Bullish headlines word cloud\n",
    "    bullish = results_df[results_df['ensemble_score'] > 0.1]['title']\n",
    "    if len(bullish) > 0:\n",
    "        bullish_text = ' '.join(bullish.tolist())\n",
    "        wc_bull = WordCloud(\n",
    "            width=800, height=400, background_color='black',\n",
    "            colormap='Greens', max_words=80\n",
    "        ).generate(bullish_text)\n",
    "        axes[0].imshow(wc_bull, interpolation='bilinear')\n",
    "        axes[0].set_title(f'Bullish Headlines ({len(bullish)} articles)', color='green', fontsize=13)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Bearish headlines word cloud\n",
    "    bearish = results_df[results_df['ensemble_score'] < -0.1]['title']\n",
    "    if len(bearish) > 0:\n",
    "        bearish_text = ' '.join(bearish.tolist())\n",
    "        wc_bear = WordCloud(\n",
    "            width=800, height=400, background_color='black',\n",
    "            colormap='Reds', max_words=80\n",
    "        ).generate(bearish_text)\n",
    "        axes[1].imshow(wc_bear, interpolation='bilinear')\n",
    "        axes[1].set_title(f'Bearish Headlines ({len(bearish)} articles)', color='red', fontsize=13)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.suptitle('Keyword Themes by Sentiment', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Word clouds reveal the dominant themes driving bullish vs bearish sentiment.\")\n",
    "    print(\"This helps you quickly understand WHAT is driving market mood.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Summary & Next Steps\n",
    "\n",
    "### What We Built\n",
    "\n",
    "| Component | Status |\n",
    "|-----------|--------|\n",
    "| RSS/Yahoo news fetching | Working |\n",
    "| Multi-model sentiment (FinBERT + Twitter-RoBERTa + DistilRoBERTa) | Working |\n",
    "| Entity/ticker extraction (keyword + NER) | Working |\n",
    "| Per-stock sentiment aggregation | Working |\n",
    "| Sentiment-price correlation framework | Working |\n",
    "| GPU throughput benchmarking | Working |\n",
    "| Trading signal generator | Working |\n",
    "| Live monitoring script (sentiment_monitor.py) | Ready to deploy |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Your 4090 processes thousands of headlines per second** -- real-time analysis is trivially fast\n",
    "2. **Multi-model ensembles are more robust** than any single model\n",
    "3. **Model disagreement is itself a signal** -- ambiguity means uncertainty\n",
    "4. **Sentiment is a complement to technical analysis**, not a replacement\n",
    "5. **The architecture scales**: same pipeline works for 10 or 10,000 headlines\n",
    "\n",
    "### Enhancement Ideas\n",
    "\n",
    "- **Add social media sources**: Reddit (r/wallstreetbets, r/stocks), Twitter/X via API\n",
    "- **Fine-tune FinBERT** on your own labeled data for better accuracy\n",
    "- **Add earnings call analysis**: Transcribe and analyze with a local LLM\n",
    "- **Sentiment decay modeling**: How long does a news event affect price?\n",
    "- **Sector-level aggregation**: Aggregate sentiment across entire sectors\n",
    "- **Contrarian signals**: Alert when sentiment is at extremes (potential reversals)\n",
    "\n",
    "### Coming Up Next\n",
    "\n",
    "**Notebook 03: Technical Strategy Backtesting** -- Build and test a complete trading strategy on historical data before risking real money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    mem_before = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    del finbert, twitter_sentiment, distil_financial, ner_pipeline\n",
    "    torch.cuda.empty_cache()\n",
    "    mem_after = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"GPU memory freed: {mem_before:.1f} GB -> {mem_after:.1f} GB\")\n",
    "\n",
    "print(\"\\nNotebook 02 complete. Your sentiment pipeline is ready for live use.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DayTrader (Python 3.10)",
   "language": "python",
   "name": "daytrader"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
